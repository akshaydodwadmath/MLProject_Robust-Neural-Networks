=> Reading YAML config from configs/configs.yml
#################### Pre-training network ####################
===>>  gradient for importance_scores: None  | training weights only
Initialization relevance score with None initialization
Files already downloaded and verified
Files already downloaded and verified
Traing loader: 50000 images, Test loader: 10000 images
Missing in RoCL, total=23, {'layer2.1.conv1.popup_scores', 'layer4.0.conv1.popup_scores', 'layer3.1.conv2.popup_scores', 'layer3.1.conv1.popup_scores', 'linear.bias', 'layer1.1.conv2.popup_scores', 'conv1.popup_scores', 'layer3.0.conv1.popup_scores', 'layer2.0.shortcut.0.popup_scores', 'layer1.0.conv1.popup_scores', 'layer4.0.conv2.popup_scores', 'linear.popup_scores', 'layer2.0.conv1.popup_scores', 'layer1.0.conv2.popup_scores', 'linear.weight', 'layer2.0.conv2.popup_scores', 'layer4.0.shortcut.0.popup_scores', 'layer4.1.conv2.popup_scores', 'layer3.0.shortcut.0.popup_scores', 'layer1.1.conv1.popup_scores', 'layer4.1.conv1.popup_scores', 'layer3.0.conv2.popup_scores', 'layer2.1.conv2.popup_scores'}
variable = conv1.weight, Gradient requires_grad = True
variable = conv1.popup_scores, Gradient requires_grad = False
variable = bn1.weight, Gradient requires_grad = True
variable = bn1.bias, Gradient requires_grad = True
variable = layer1.0.conv1.weight, Gradient requires_grad = True
variable = layer1.0.conv1.popup_scores, Gradient requires_grad = False
variable = layer1.0.bn1.weight, Gradient requires_grad = True
variable = layer1.0.bn1.bias, Gradient requires_grad = True
variable = layer1.0.conv2.weight, Gradient requires_grad = True
variable = layer1.0.conv2.popup_scores, Gradient requires_grad = False
variable = layer1.0.bn2.weight, Gradient requires_grad = True
variable = layer1.0.bn2.bias, Gradient requires_grad = True
variable = layer1.1.conv1.weight, Gradient requires_grad = True
variable = layer1.1.conv1.popup_scores, Gradient requires_grad = False
variable = layer1.1.bn1.weight, Gradient requires_grad = True
variable = layer1.1.bn1.bias, Gradient requires_grad = True
variable = layer1.1.conv2.weight, Gradient requires_grad = True
variable = layer1.1.conv2.popup_scores, Gradient requires_grad = False
variable = layer1.1.bn2.weight, Gradient requires_grad = True
variable = layer1.1.bn2.bias, Gradient requires_grad = True
variable = layer2.0.conv1.weight, Gradient requires_grad = True
variable = layer2.0.conv1.popup_scores, Gradient requires_grad = False
variable = layer2.0.bn1.weight, Gradient requires_grad = True
variable = layer2.0.bn1.bias, Gradient requires_grad = True
variable = layer2.0.conv2.weight, Gradient requires_grad = True
variable = layer2.0.conv2.popup_scores, Gradient requires_grad = False
variable = layer2.0.bn2.weight, Gradient requires_grad = True
variable = layer2.0.bn2.bias, Gradient requires_grad = True
variable = layer2.0.shortcut.0.weight, Gradient requires_grad = True
variable = layer2.0.shortcut.0.popup_scores, Gradient requires_grad = False
variable = layer2.0.shortcut.1.weight, Gradient requires_grad = True
variable = layer2.0.shortcut.1.bias, Gradient requires_grad = True
variable = layer2.1.conv1.weight, Gradient requires_grad = True
variable = layer2.1.conv1.popup_scores, Gradient requires_grad = False
variable = layer2.1.bn1.weight, Gradient requires_grad = True
variable = layer2.1.bn1.bias, Gradient requires_grad = True
variable = layer2.1.conv2.weight, Gradient requires_grad = True
variable = layer2.1.conv2.popup_scores, Gradient requires_grad = False
variable = layer2.1.bn2.weight, Gradient requires_grad = True
variable = layer2.1.bn2.bias, Gradient requires_grad = True
variable = layer3.0.conv1.weight, Gradient requires_grad = True
variable = layer3.0.conv1.popup_scores, Gradient requires_grad = False
variable = layer3.0.bn1.weight, Gradient requires_grad = True
variable = layer3.0.bn1.bias, Gradient requires_grad = True
variable = layer3.0.conv2.weight, Gradient requires_grad = True
variable = layer3.0.conv2.popup_scores, Gradient requires_grad = False
variable = layer3.0.bn2.weight, Gradient requires_grad = True
variable = layer3.0.bn2.bias, Gradient requires_grad = True
variable = layer3.0.shortcut.0.weight, Gradient requires_grad = True
variable = layer3.0.shortcut.0.popup_scores, Gradient requires_grad = False
variable = layer3.0.shortcut.1.weight, Gradient requires_grad = True
variable = layer3.0.shortcut.1.bias, Gradient requires_grad = True
variable = layer3.1.conv1.weight, Gradient requires_grad = True
variable = layer3.1.conv1.popup_scores, Gradient requires_grad = False
variable = layer3.1.bn1.weight, Gradient requires_grad = True
variable = layer3.1.bn1.bias, Gradient requires_grad = True
variable = layer3.1.conv2.weight, Gradient requires_grad = True
variable = layer3.1.conv2.popup_scores, Gradient requires_grad = False
variable = layer3.1.bn2.weight, Gradient requires_grad = True
variable = layer3.1.bn2.bias, Gradient requires_grad = True
variable = layer4.0.conv1.weight, Gradient requires_grad = True
variable = layer4.0.conv1.popup_scores, Gradient requires_grad = False
variable = layer4.0.bn1.weight, Gradient requires_grad = True
variable = layer4.0.bn1.bias, Gradient requires_grad = True
variable = layer4.0.conv2.weight, Gradient requires_grad = True
variable = layer4.0.conv2.popup_scores, Gradient requires_grad = False
variable = layer4.0.bn2.weight, Gradient requires_grad = True
variable = layer4.0.bn2.bias, Gradient requires_grad = True
variable = layer4.0.shortcut.0.weight, Gradient requires_grad = True
variable = layer4.0.shortcut.0.popup_scores, Gradient requires_grad = False
variable = layer4.0.shortcut.1.weight, Gradient requires_grad = True
variable = layer4.0.shortcut.1.bias, Gradient requires_grad = True
variable = layer4.1.conv1.weight, Gradient requires_grad = True
variable = layer4.1.conv1.popup_scores, Gradient requires_grad = False
variable = layer4.1.bn1.weight, Gradient requires_grad = True
variable = layer4.1.bn1.bias, Gradient requires_grad = True
variable = layer4.1.conv2.weight, Gradient requires_grad = True
variable = layer4.1.conv2.popup_scores, Gradient requires_grad = False
variable = layer4.1.bn2.weight, Gradient requires_grad = True
variable = layer4.1.bn2.bias, Gradient requires_grad = True
variable = linear.weight, Gradient requires_grad = True
variable = linear.bias, Gradient requires_grad = True
variable = linear.popup_scores, Gradient requires_grad = False
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.09961
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [0][  0/196]	Time  3.733 ( 3.733)	Data  0.000 ( 0.000)	Loss 2.3551 (2.3551)	Acc_1   5.08 (  5.08)	Acc_5  49.22 ( 49.22)
Epoch: [0][100/196]	Time  3.672 ( 3.674)	Data  0.000 ( 0.000)	Loss 1.6397 (1.5900)	Acc_1  62.11 ( 59.96)	Acc_5  94.53 ( 94.06)
Test: [39/40]	Time  0.539 ( 2.293)	Loss 0.9764 (1.1536)	Adv_Loss 1.3971 (1.6204)	Acc_1  75.00 ( 66.43)	Acc_5 100.00 ( 96.50)	Adv-Acc_1  43.75 ( 40.62)	Adv-Acc_5  93.75 ( 88.39)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.10000
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [1][  0/196]	Time  3.510 ( 3.510)	Data  0.000 ( 0.000)	Loss 1.4522 (1.4522)	Acc_1  64.06 ( 64.06)	Acc_5  97.66 ( 97.66)
Epoch: [1][100/196]	Time  3.676 ( 3.677)	Data  0.000 ( 0.000)	Loss 1.4482 (1.4863)	Acc_1  69.14 ( 66.54)	Acc_5  96.88 ( 96.68)
Test: [39/40]	Time  0.552 ( 2.300)	Loss 0.8687 (1.0925)	Adv_Loss 1.2796 (1.5541)	Acc_1  75.00 ( 69.98)	Acc_5 100.00 ( 97.63)	Adv-Acc_1  50.00 ( 42.49)	Adv-Acc_5  93.75 ( 90.32)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.09961
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [2][  0/196]	Time  3.485 ( 3.485)	Data  0.000 ( 0.000)	Loss 1.3815 (1.3815)	Acc_1  70.31 ( 70.31)	Acc_5  96.88 ( 96.88)
Epoch: [2][100/196]	Time  3.673 ( 3.674)	Data  0.000 ( 0.000)	Loss 1.5152 (1.4261)	Acc_1  64.06 ( 68.63)	Acc_5  96.88 ( 97.33)
Test: [39/40]	Time  0.529 ( 2.292)	Loss 0.8725 (1.0774)	Adv_Loss 1.2770 (1.5346)	Acc_1  93.75 ( 69.88)	Acc_5 100.00 ( 97.54)	Adv-Acc_1  37.50 ( 42.51)	Adv-Acc_5  93.75 ( 90.47)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.09843
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [3][  0/196]	Time  3.466 ( 3.466)	Data  0.000 ( 0.000)	Loss 1.3851 (1.3851)	Acc_1  71.09 ( 71.09)	Acc_5  97.27 ( 97.27)
Epoch: [3][100/196]	Time  3.693 ( 3.673)	Data  0.000 ( 0.000)	Loss 1.3606 (1.3956)	Acc_1  74.22 ( 70.08)	Acc_5  96.88 ( 97.45)
Test: [39/40]	Time  0.528 ( 2.294)	Loss 0.8019 (1.0263)	Adv_Loss 1.2053 (1.4965)	Acc_1  81.25 ( 72.96)	Acc_5 100.00 ( 98.11)	Adv-Acc_1  56.25 ( 44.75)	Adv-Acc_5 100.00 ( 91.20)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.09649
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [4][  0/196]	Time  3.516 ( 3.516)	Data  0.000 ( 0.000)	Loss 1.4106 (1.4106)	Acc_1  67.97 ( 67.97)	Acc_5  96.48 ( 96.48)
Epoch: [4][100/196]	Time  3.673 ( 3.680)	Data  0.000 ( 0.000)	Loss 1.3645 (1.3711)	Acc_1  71.48 ( 71.42)	Acc_5  96.48 ( 97.75)
Test: [39/40]	Time  0.540 ( 2.295)	Loss 0.8371 (1.0336)	Adv_Loss 1.2385 (1.5004)	Acc_1  75.00 ( 72.29)	Acc_5 100.00 ( 98.04)	Adv-Acc_1  62.50 ( 44.60)	Adv-Acc_5  93.75 ( 91.19)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.09382
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [5][  0/196]	Time  3.441 ( 3.441)	Data  0.000 ( 0.000)	Loss 1.3864 (1.3864)	Acc_1  72.66 ( 72.66)	Acc_5  97.66 ( 97.66)
Epoch: [5][100/196]	Time  3.676 ( 3.676)	Data  0.000 ( 0.000)	Loss 1.3275 (1.3540)	Acc_1  72.27 ( 72.16)	Acc_5  96.88 ( 97.85)
Test: [39/40]	Time  0.552 ( 2.300)	Loss 0.8118 (1.0330)	Adv_Loss 1.1907 (1.4856)	Acc_1  81.25 ( 72.42)	Acc_5 100.00 ( 98.07)	Adv-Acc_1  56.25 ( 44.90)	Adv-Acc_5 100.00 ( 91.81)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.09045
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [6][  0/196]	Time  3.513 ( 3.513)	Data  0.000 ( 0.000)	Loss 1.2830 (1.2830)	Acc_1  74.61 ( 74.61)	Acc_5  98.44 ( 98.44)
Epoch: [6][100/196]	Time  3.676 ( 3.676)	Data  0.000 ( 0.000)	Loss 1.3275 (1.3285)	Acc_1  71.09 ( 73.67)	Acc_5  98.83 ( 98.03)
Test: [39/40]	Time  0.536 ( 2.293)	Loss 0.8072 (0.9950)	Adv_Loss 1.1991 (1.4635)	Acc_1  87.50 ( 73.36)	Acc_5 100.00 ( 98.22)	Adv-Acc_1  43.75 ( 45.61)	Adv-Acc_5 100.00 ( 91.82)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.08645
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [7][  0/196]	Time  3.432 ( 3.432)	Data  0.000 ( 0.000)	Loss 1.2881 (1.2881)	Acc_1  78.12 ( 78.12)	Acc_5 100.00 (100.00)
Epoch: [7][100/196]	Time  3.679 ( 3.676)	Data  0.000 ( 0.000)	Loss 1.3270 (1.3133)	Acc_1  78.12 ( 74.37)	Acc_5  98.83 ( 98.23)
Test: [39/40]	Time  0.550 ( 2.309)	Loss 0.8399 (0.9948)	Adv_Loss 1.2354 (1.4579)	Acc_1  87.50 ( 73.36)	Acc_5 100.00 ( 98.14)	Adv-Acc_1  37.50 ( 45.92)	Adv-Acc_5 100.00 ( 91.57)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.08187
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [8][  0/196]	Time  3.524 ( 3.524)	Data  0.000 ( 0.000)	Loss 1.3001 (1.3001)	Acc_1  70.70 ( 70.70)	Acc_5  98.44 ( 98.44)
Epoch: [8][100/196]	Time  3.667 ( 3.676)	Data  0.000 ( 0.000)	Loss 1.3429 (1.3015)	Acc_1  71.88 ( 74.90)	Acc_5  98.05 ( 98.14)
Test: [39/40]	Time  0.529 ( 2.292)	Loss 0.8363 (0.9807)	Adv_Loss 1.2290 (1.4507)	Acc_1  87.50 ( 73.71)	Acc_5 100.00 ( 98.14)	Adv-Acc_1  50.00 ( 46.75)	Adv-Acc_5 100.00 ( 91.76)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.07679
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [9][  0/196]	Time  3.430 ( 3.430)	Data  0.000 ( 0.000)	Loss 1.2668 (1.2668)	Acc_1  75.00 ( 75.00)	Acc_5  98.83 ( 98.83)
Epoch: [9][100/196]	Time  3.698 ( 3.686)	Data  0.000 ( 0.000)	Loss 1.3777 (1.2922)	Acc_1  70.31 ( 75.17)	Acc_5  97.66 ( 98.30)
Test: [39/40]	Time  0.530 ( 2.295)	Loss 0.7986 (0.9747)	Adv_Loss 1.1935 (1.4474)	Acc_1  81.25 ( 74.08)	Acc_5 100.00 ( 98.10)	Adv-Acc_1  62.50 ( 46.76)	Adv-Acc_5  93.75 ( 91.70)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.07129
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [10][  0/196]	Time  3.483 ( 3.483)	Data  0.000 ( 0.000)	Loss 1.2621 (1.2621)	Acc_1  75.39 ( 75.39)	Acc_5  98.44 ( 98.44)
Epoch: [10][100/196]	Time  3.673 ( 3.675)	Data  0.000 ( 0.000)	Loss 1.3777 (1.2798)	Acc_1  73.83 ( 75.71)	Acc_5  98.05 ( 98.26)
Test: [39/40]	Time  0.532 ( 2.295)	Loss 0.7326 (0.9512)	Adv_Loss 1.1313 (1.4275)	Acc_1  87.50 ( 74.98)	Acc_5 100.00 ( 98.38)	Adv-Acc_1  62.50 ( 47.76)	Adv-Acc_5 100.00 ( 92.05)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.06545
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [11][  0/196]	Time  3.528 ( 3.528)	Data  0.000 ( 0.000)	Loss 1.2044 (1.2044)	Acc_1  78.91 ( 78.91)	Acc_5  99.61 ( 99.61)
Epoch: [11][100/196]	Time  3.697 ( 3.680)	Data  0.000 ( 0.000)	Loss 1.3296 (1.2647)	Acc_1  74.61 ( 76.81)	Acc_5  99.22 ( 98.38)
Test: [39/40]	Time  0.532 ( 2.292)	Loss 0.7542 (0.9596)	Adv_Loss 1.1352 (1.4336)	Acc_1  87.50 ( 74.98)	Acc_5 100.00 ( 98.38)	Adv-Acc_1  50.00 ( 46.96)	Adv-Acc_5  93.75 ( 92.32)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.05937
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [12][  0/196]	Time  3.471 ( 3.471)	Data  0.000 ( 0.000)	Loss 1.2049 (1.2049)	Acc_1  77.73 ( 77.73)	Acc_5  99.22 ( 99.22)
Epoch: [12][100/196]	Time  3.681 ( 3.673)	Data  0.000 ( 0.000)	Loss 1.2039 (1.2562)	Acc_1  78.12 ( 76.97)	Acc_5  98.44 ( 98.51)
Test: [39/40]	Time  0.545 ( 2.308)	Loss 0.7703 (0.9517)	Adv_Loss 1.1537 (1.4196)	Acc_1  87.50 ( 75.73)	Acc_5 100.00 ( 98.39)	Adv-Acc_1  56.25 ( 47.67)	Adv-Acc_5 100.00 ( 92.33)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.05314
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [13][  0/196]	Time  3.480 ( 3.480)	Data  0.000 ( 0.000)	Loss 1.2352 (1.2352)	Acc_1  75.39 ( 75.39)	Acc_5  99.22 ( 99.22)
Epoch: [13][100/196]	Time  3.690 ( 3.679)	Data  0.000 ( 0.000)	Loss 1.2497 (1.2519)	Acc_1  74.22 ( 77.43)	Acc_5  99.22 ( 98.51)
Test: [39/40]	Time  0.527 ( 2.298)	Loss 0.7329 (0.9415)	Adv_Loss 1.1234 (1.4222)	Acc_1  87.50 ( 75.53)	Acc_5 100.00 ( 98.42)	Adv-Acc_1  56.25 ( 47.33)	Adv-Acc_5 100.00 ( 92.32)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.04686
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [14][  0/196]	Time  3.437 ( 3.437)	Data  0.000 ( 0.000)	Loss 1.2308 (1.2308)	Acc_1  80.47 ( 80.47)	Acc_5  99.22 ( 99.22)
Epoch: [14][100/196]	Time  3.673 ( 3.675)	Data  0.000 ( 0.000)	Loss 1.2249 (1.2361)	Acc_1  77.73 ( 78.21)	Acc_5  98.83 ( 98.72)
Test: [39/40]	Time  0.532 ( 2.297)	Loss 0.7096 (0.9436)	Adv_Loss 1.0909 (1.4188)	Acc_1  87.50 ( 75.77)	Acc_5 100.00 ( 98.48)	Adv-Acc_1  62.50 ( 47.49)	Adv-Acc_5 100.00 ( 92.04)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.04063
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [15][  0/196]	Time  3.479 ( 3.479)	Data  0.000 ( 0.000)	Loss 1.1510 (1.1510)	Acc_1  79.30 ( 79.30)	Acc_5  99.61 ( 99.61)
Epoch: [15][100/196]	Time  3.696 ( 3.681)	Data  0.000 ( 0.000)	Loss 1.2791 (1.2331)	Acc_1  76.17 ( 78.25)	Acc_5  99.61 ( 98.69)
Test: [39/40]	Time  0.525 ( 2.291)	Loss 0.7693 (0.9432)	Adv_Loss 1.1597 (1.4183)	Acc_1  81.25 ( 75.51)	Acc_5 100.00 ( 98.48)	Adv-Acc_1  56.25 ( 47.89)	Adv-Acc_5  93.75 ( 92.18)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.03455
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [16][  0/196]	Time  3.432 ( 3.432)	Data  0.000 ( 0.000)	Loss 1.1716 (1.1716)	Acc_1  80.08 ( 80.08)	Acc_5  99.61 ( 99.61)
Epoch: [16][100/196]	Time  3.674 ( 3.674)	Data  0.000 ( 0.000)	Loss 1.2000 (1.2215)	Acc_1  80.86 ( 78.67)	Acc_5  99.22 ( 98.69)
Test: [39/40]	Time  0.529 ( 2.294)	Loss 0.7421 (0.9143)	Adv_Loss 1.1335 (1.4022)	Acc_1  87.50 ( 76.56)	Acc_5 100.00 ( 98.49)	Adv-Acc_1  56.25 ( 48.36)	Adv-Acc_5 100.00 ( 92.16)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.02871
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [17][  0/196]	Time  3.470 ( 3.470)	Data  0.000 ( 0.000)	Loss 1.2279 (1.2279)	Acc_1  79.69 ( 79.69)	Acc_5  99.61 ( 99.61)
Epoch: [17][100/196]	Time  3.670 ( 3.672)	Data  0.000 ( 0.000)	Loss 1.2721 (1.2122)	Acc_1  75.00 ( 79.25)	Acc_5  97.66 ( 98.92)
Test: [39/40]	Time  0.546 ( 2.308)	Loss 0.7300 (0.9288)	Adv_Loss 1.1133 (1.4060)	Acc_1  87.50 ( 76.66)	Acc_5 100.00 ( 98.43)	Adv-Acc_1  56.25 ( 48.22)	Adv-Acc_5 100.00 ( 92.22)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.02321
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [18][  0/196]	Time  3.464 ( 3.464)	Data  0.000 ( 0.000)	Loss 1.2230 (1.2230)	Acc_1  82.42 ( 82.42)	Acc_5  97.27 ( 97.27)
Epoch: [18][100/196]	Time  3.695 ( 3.696)	Data  0.000 ( 0.000)	Loss 1.1606 (1.2140)	Acc_1  83.59 ( 79.36)	Acc_5  99.22 ( 98.77)
Test: [39/40]	Time  0.530 ( 2.294)	Loss 0.7363 (0.9142)	Adv_Loss 1.1326 (1.3966)	Acc_1  87.50 ( 76.53)	Acc_5 100.00 ( 98.43)	Adv-Acc_1  62.50 ( 48.76)	Adv-Acc_5 100.00 ( 92.11)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.01813
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [19][  0/196]	Time  3.474 ( 3.474)	Data  0.000 ( 0.000)	Loss 1.2141 (1.2141)	Acc_1  80.47 ( 80.47)	Acc_5  98.44 ( 98.44)
Epoch: [19][100/196]	Time  3.675 ( 3.675)	Data  0.000 ( 0.000)	Loss 1.2691 (1.2012)	Acc_1  77.34 ( 80.01)	Acc_5  99.22 ( 98.83)
Test: [39/40]	Time  0.531 ( 2.294)	Loss 0.7266 (0.9150)	Adv_Loss 1.1141 (1.3940)	Acc_1  87.50 ( 76.78)	Acc_5 100.00 ( 98.47)	Adv-Acc_1  56.25 ( 49.21)	Adv-Acc_5 100.00 ( 92.29)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.01355
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [20][  0/196]	Time  3.479 ( 3.479)	Data  0.000 ( 0.000)	Loss 1.1699 (1.1699)	Acc_1  81.64 ( 81.64)	Acc_5  99.61 ( 99.61)
Epoch: [20][100/196]	Time  3.675 ( 3.674)	Data  0.000 ( 0.000)	Loss 1.2366 (1.1997)	Acc_1  77.73 ( 80.04)	Acc_5  98.83 ( 98.83)
Test: [39/40]	Time  0.551 ( 2.306)	Loss 0.7239 (0.9037)	Adv_Loss 1.1443 (1.3921)	Acc_1  87.50 ( 77.06)	Acc_5 100.00 ( 98.55)	Adv-Acc_1  56.25 ( 48.43)	Adv-Acc_5  93.75 ( 92.34)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.00955
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [21][  0/196]	Time  3.468 ( 3.468)	Data  0.000 ( 0.000)	Loss 1.1403 (1.1403)	Acc_1  81.64 ( 81.64)	Acc_5 100.00 (100.00)
Epoch: [21][100/196]	Time  3.669 ( 3.674)	Data  0.000 ( 0.000)	Loss 1.0996 (1.1911)	Acc_1  85.16 ( 80.36)	Acc_5  99.22 ( 98.94)
Test: [39/40]	Time  0.528 ( 2.292)	Loss 0.7023 (0.9133)	Adv_Loss 1.0883 (1.3932)	Acc_1  87.50 ( 77.06)	Acc_5 100.00 ( 98.50)	Adv-Acc_1  56.25 ( 48.59)	Adv-Acc_5 100.00 ( 92.21)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.00618
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [22][  0/196]	Time  3.516 ( 3.516)	Data  0.000 ( 0.000)	Loss 1.1723 (1.1723)	Acc_1  82.03 ( 82.03)	Acc_5  98.83 ( 98.83)
Epoch: [22][100/196]	Time  3.678 ( 3.679)	Data  0.000 ( 0.000)	Loss 1.1900 (1.1912)	Acc_1  79.30 ( 80.29)	Acc_5  98.83 ( 98.90)
Test: [39/40]	Time  0.543 ( 2.297)	Loss 0.7160 (0.9056)	Adv_Loss 1.1113 (1.3893)	Acc_1  87.50 ( 76.98)	Acc_5 100.00 ( 98.55)	Adv-Acc_1  56.25 ( 48.92)	Adv-Acc_5 100.00 ( 92.33)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.00351
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [23][  0/196]	Time  3.455 ( 3.455)	Data  0.000 ( 0.000)	Loss 1.1940 (1.1940)	Acc_1  80.86 ( 80.86)	Acc_5  98.83 ( 98.83)
Epoch: [23][100/196]	Time  3.683 ( 3.678)	Data  0.000 ( 0.000)	Loss 1.1758 (1.1952)	Acc_1  82.42 ( 80.40)	Acc_5  98.83 ( 98.96)
Test: [39/40]	Time  0.541 ( 2.296)	Loss 0.7041 (0.9022)	Adv_Loss 1.0911 (1.3874)	Acc_1  87.50 ( 77.30)	Acc_5 100.00 ( 98.53)	Adv-Acc_1  56.25 ( 48.53)	Adv-Acc_5 100.00 ( 92.44)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.00157
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [24][  0/196]	Time  3.481 ( 3.481)	Data  0.000 ( 0.000)	Loss 1.1751 (1.1751)	Acc_1  83.98 ( 83.98)	Acc_5  98.83 ( 98.83)
Epoch: [24][100/196]	Time  3.685 ( 3.680)	Data  0.000 ( 0.000)	Loss 1.2053 (1.1895)	Acc_1  80.86 ( 80.52)	Acc_5  98.83 ( 98.98)
Test: [39/40]	Time  0.549 ( 2.299)	Loss 0.7018 (0.9047)	Adv_Loss 1.0897 (1.3900)	Acc_1  87.50 ( 77.36)	Acc_5 100.00 ( 98.51)	Adv-Acc_1  56.25 ( 48.81)	Adv-Acc_5 100.00 ( 92.19)
conv1 SubnetConv(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 0.0
layer1.0.conv1 SubnetConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 0.0
layer1.0.conv2 SubnetConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 0.0
layer1.1.conv1 SubnetConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 0.0
layer1.1.conv2 SubnetConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 0.0
layer2.0.conv1 SubnetConv(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) 0.0
layer2.0.conv2 SubnetConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 0.0
layer2.0.shortcut.0 SubnetConv(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) 0.0
layer2.1.conv1 SubnetConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 0.0
layer2.1.conv2 SubnetConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 0.0
layer3.0.conv1 SubnetConv(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) 0.0
layer3.0.conv2 SubnetConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 0.0
layer3.0.shortcut.0 SubnetConv(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) 0.0
layer3.1.conv1 SubnetConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 0.0
layer3.1.conv2 SubnetConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 0.0
layer4.0.conv1 SubnetConv(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) 0.0
layer4.0.conv2 SubnetConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 0.0
layer4.0.shortcut.0 SubnetConv(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) 0.0
layer4.1.conv1 SubnetConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 0.0
layer4.1.conv2 SubnetConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 0.0
linear SubnetLinear(in_features=512, out_features=10, bias=True) 0.0
=> Reading YAML config from configs/configs.yml
#################### Pruning network ####################
===>>  gradient for weights: None  | training importance scores only
Initialization relevance score with None initialization
Files already downloaded and verified
Files already downloaded and verified
Traing loader: 50000 images, Test loader: 10000 images
Initialization relevance score proportional to weight magnitudes (OVERWRITING SOURCE NET SCORES)
Test: [39/40]	Time  0.557 ( 2.313)	Loss 2.6123 (2.6589)	Adv_Loss 2.6666 (2.7118)	Acc_1  18.75 ( 10.00)	Acc_5  62.50 ( 53.13)	Adv-Acc_1  18.75 ( 10.00)	Adv-Acc_5  62.50 ( 49.63)
variable = conv1.weight, Gradient requires_grad = False
variable = conv1.popup_scores, Gradient requires_grad = True
variable = bn1.weight, Gradient requires_grad = True
variable = bn1.bias, Gradient requires_grad = True
variable = layer1.0.conv1.weight, Gradient requires_grad = False
variable = layer1.0.conv1.popup_scores, Gradient requires_grad = True
variable = layer1.0.bn1.weight, Gradient requires_grad = True
variable = layer1.0.bn1.bias, Gradient requires_grad = True
variable = layer1.0.conv2.weight, Gradient requires_grad = False
variable = layer1.0.conv2.popup_scores, Gradient requires_grad = True
variable = layer1.0.bn2.weight, Gradient requires_grad = True
variable = layer1.0.bn2.bias, Gradient requires_grad = True
variable = layer1.1.conv1.weight, Gradient requires_grad = False
variable = layer1.1.conv1.popup_scores, Gradient requires_grad = True
variable = layer1.1.bn1.weight, Gradient requires_grad = True
variable = layer1.1.bn1.bias, Gradient requires_grad = True
variable = layer1.1.conv2.weight, Gradient requires_grad = False
variable = layer1.1.conv2.popup_scores, Gradient requires_grad = True
variable = layer1.1.bn2.weight, Gradient requires_grad = True
variable = layer1.1.bn2.bias, Gradient requires_grad = True
variable = layer2.0.conv1.weight, Gradient requires_grad = False
variable = layer2.0.conv1.popup_scores, Gradient requires_grad = True
variable = layer2.0.bn1.weight, Gradient requires_grad = True
variable = layer2.0.bn1.bias, Gradient requires_grad = True
variable = layer2.0.conv2.weight, Gradient requires_grad = False
variable = layer2.0.conv2.popup_scores, Gradient requires_grad = True
variable = layer2.0.bn2.weight, Gradient requires_grad = True
variable = layer2.0.bn2.bias, Gradient requires_grad = True
variable = layer2.0.shortcut.0.weight, Gradient requires_grad = False
variable = layer2.0.shortcut.0.popup_scores, Gradient requires_grad = True
variable = layer2.0.shortcut.1.weight, Gradient requires_grad = True
variable = layer2.0.shortcut.1.bias, Gradient requires_grad = True
variable = layer2.1.conv1.weight, Gradient requires_grad = False
variable = layer2.1.conv1.popup_scores, Gradient requires_grad = True
variable = layer2.1.bn1.weight, Gradient requires_grad = True
variable = layer2.1.bn1.bias, Gradient requires_grad = True
variable = layer2.1.conv2.weight, Gradient requires_grad = False
variable = layer2.1.conv2.popup_scores, Gradient requires_grad = True
variable = layer2.1.bn2.weight, Gradient requires_grad = True
variable = layer2.1.bn2.bias, Gradient requires_grad = True
variable = layer3.0.conv1.weight, Gradient requires_grad = False
variable = layer3.0.conv1.popup_scores, Gradient requires_grad = True
variable = layer3.0.bn1.weight, Gradient requires_grad = True
variable = layer3.0.bn1.bias, Gradient requires_grad = True
variable = layer3.0.conv2.weight, Gradient requires_grad = False
variable = layer3.0.conv2.popup_scores, Gradient requires_grad = True
variable = layer3.0.bn2.weight, Gradient requires_grad = True
variable = layer3.0.bn2.bias, Gradient requires_grad = True
variable = layer3.0.shortcut.0.weight, Gradient requires_grad = False
variable = layer3.0.shortcut.0.popup_scores, Gradient requires_grad = True
variable = layer3.0.shortcut.1.weight, Gradient requires_grad = True
variable = layer3.0.shortcut.1.bias, Gradient requires_grad = True
variable = layer3.1.conv1.weight, Gradient requires_grad = False
variable = layer3.1.conv1.popup_scores, Gradient requires_grad = True
variable = layer3.1.bn1.weight, Gradient requires_grad = True
variable = layer3.1.bn1.bias, Gradient requires_grad = True
variable = layer3.1.conv2.weight, Gradient requires_grad = False
variable = layer3.1.conv2.popup_scores, Gradient requires_grad = True
variable = layer3.1.bn2.weight, Gradient requires_grad = True
variable = layer3.1.bn2.bias, Gradient requires_grad = True
variable = layer4.0.conv1.weight, Gradient requires_grad = False
variable = layer4.0.conv1.popup_scores, Gradient requires_grad = True
variable = layer4.0.bn1.weight, Gradient requires_grad = True
variable = layer4.0.bn1.bias, Gradient requires_grad = True
variable = layer4.0.conv2.weight, Gradient requires_grad = False
variable = layer4.0.conv2.popup_scores, Gradient requires_grad = True
variable = layer4.0.bn2.weight, Gradient requires_grad = True
variable = layer4.0.bn2.bias, Gradient requires_grad = True
variable = layer4.0.shortcut.0.weight, Gradient requires_grad = False
variable = layer4.0.shortcut.0.popup_scores, Gradient requires_grad = True
variable = layer4.0.shortcut.1.weight, Gradient requires_grad = True
variable = layer4.0.shortcut.1.bias, Gradient requires_grad = True
variable = layer4.1.conv1.weight, Gradient requires_grad = False
variable = layer4.1.conv1.popup_scores, Gradient requires_grad = True
variable = layer4.1.bn1.weight, Gradient requires_grad = True
variable = layer4.1.bn1.bias, Gradient requires_grad = True
variable = layer4.1.conv2.weight, Gradient requires_grad = False
variable = layer4.1.conv2.popup_scores, Gradient requires_grad = True
variable = layer4.1.bn2.weight, Gradient requires_grad = True
variable = layer4.1.bn2.bias, Gradient requires_grad = True
variable = linear.weight, Gradient requires_grad = False
variable = linear.bias, Gradient requires_grad = False
variable = linear.popup_scores, Gradient requires_grad = True
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.09961
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [0][  0/196]	Time  3.454 ( 3.454)	Data  0.000 ( 0.000)	Loss 1.7350 (1.7350)	Acc_1  56.64 ( 56.64)	Acc_5  94.92 ( 94.92)
Epoch: [0][100/196]	Time  3.679 ( 3.681)	Data  0.000 ( 0.000)	Loss 1.4419 (1.5076)	Acc_1  70.31 ( 68.47)	Acc_5  98.83 ( 97.41)
Test: [39/40]	Time  0.551 ( 2.304)	Loss 0.8866 (1.0859)	Adv_Loss 1.2816 (1.5618)	Acc_1  81.25 ( 69.67)	Acc_5 100.00 ( 97.74)	Adv-Acc_1  50.00 ( 42.53)	Adv-Acc_5 100.00 ( 89.79)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.10000
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [1][  0/196]	Time  3.533 ( 3.533)	Data  0.000 ( 0.000)	Loss 1.3675 (1.3675)	Acc_1  65.62 ( 65.62)	Acc_5  97.66 ( 97.66)
Epoch: [1][100/196]	Time  3.685 ( 3.683)	Data  0.000 ( 0.000)	Loss 1.2753 (1.3670)	Acc_1  73.44 ( 72.18)	Acc_5  97.66 ( 97.95)
Test: [39/40]	Time  0.531 ( 2.297)	Loss 0.9489 (1.0688)	Adv_Loss 1.4188 (1.5609)	Acc_1  68.75 ( 70.77)	Acc_5 100.00 ( 97.72)	Adv-Acc_1  62.50 ( 42.28)	Adv-Acc_5 100.00 ( 90.04)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.09961
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [2][  0/196]	Time  3.510 ( 3.510)	Data  0.000 ( 0.000)	Loss 1.3728 (1.3728)	Acc_1  72.66 ( 72.66)	Acc_5  98.44 ( 98.44)
Epoch: [2][100/196]	Time  3.675 ( 3.674)	Data  0.000 ( 0.000)	Loss 1.3327 (1.3437)	Acc_1  68.36 ( 73.37)	Acc_5  98.05 ( 97.97)
Test: [39/40]	Time  0.530 ( 2.297)	Loss 0.8420 (1.0126)	Adv_Loss 1.3123 (1.5302)	Acc_1  87.50 ( 72.60)	Acc_5 100.00 ( 98.02)	Adv-Acc_1  25.00 ( 42.44)	Adv-Acc_5  93.75 ( 91.00)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.09843
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [3][  0/196]	Time  3.449 ( 3.449)	Data  0.000 ( 0.000)	Loss 1.2377 (1.2377)	Acc_1  76.56 ( 76.56)	Acc_5  98.44 ( 98.44)
Epoch: [3][100/196]	Time  3.672 ( 3.673)	Data  0.000 ( 0.000)	Loss 1.3011 (1.3268)	Acc_1  76.17 ( 74.14)	Acc_5  97.66 ( 98.10)
Test: [39/40]	Time  0.530 ( 2.297)	Loss 0.8194 (1.0349)	Adv_Loss 1.2210 (1.5057)	Acc_1  81.25 ( 72.67)	Acc_5 100.00 ( 97.98)	Adv-Acc_1  50.00 ( 43.92)	Adv-Acc_5 100.00 ( 91.03)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.09649
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [4][  0/196]	Time  3.479 ( 3.479)	Data  0.000 ( 0.000)	Loss 1.2986 (1.2986)	Acc_1  76.56 ( 76.56)	Acc_5  98.05 ( 98.05)
Epoch: [4][100/196]	Time  3.679 ( 3.677)	Data  0.000 ( 0.000)	Loss 1.3415 (1.3096)	Acc_1  69.14 ( 74.73)	Acc_5  96.09 ( 98.21)
Test: [39/40]	Time  0.536 ( 2.301)	Loss 0.8462 (1.0267)	Adv_Loss 1.2615 (1.5105)	Acc_1  75.00 ( 72.82)	Acc_5 100.00 ( 98.11)	Adv-Acc_1  56.25 ( 43.78)	Adv-Acc_5 100.00 ( 91.11)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.09382
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [5][  0/196]	Time  3.493 ( 3.493)	Data  0.000 ( 0.000)	Loss 1.2257 (1.2257)	Acc_1  80.86 ( 80.86)	Acc_5  99.22 ( 99.22)
Epoch: [5][100/196]	Time  3.677 ( 3.674)	Data  0.000 ( 0.000)	Loss 1.3793 (1.2943)	Acc_1  74.22 ( 75.41)	Acc_5  97.66 ( 98.50)
Test: [39/40]	Time  0.530 ( 2.297)	Loss 0.8318 (1.0025)	Adv_Loss 1.2544 (1.5019)	Acc_1  81.25 ( 74.21)	Acc_5 100.00 ( 98.38)	Adv-Acc_1  56.25 ( 44.51)	Adv-Acc_5 100.00 ( 91.17)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.09045
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [6][  0/196]	Time  3.490 ( 3.490)	Data  0.000 ( 0.000)	Loss 1.2233 (1.2233)	Acc_1  77.34 ( 77.34)	Acc_5  97.66 ( 97.66)
Epoch: [6][100/196]	Time  3.672 ( 3.674)	Data  0.000 ( 0.000)	Loss 1.3431 (1.2886)	Acc_1  73.83 ( 75.50)	Acc_5  96.88 ( 98.39)
Test: [39/40]	Time  0.533 ( 2.297)	Loss 0.8721 (1.0207)	Adv_Loss 1.3277 (1.5267)	Acc_1  75.00 ( 71.17)	Acc_5 100.00 ( 97.85)	Adv-Acc_1  50.00 ( 42.94)	Adv-Acc_5 100.00 ( 90.71)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.08645
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [7][  0/196]	Time  3.438 ( 3.438)	Data  0.000 ( 0.000)	Loss 1.2717 (1.2717)	Acc_1  73.05 ( 73.05)	Acc_5  99.22 ( 99.22)
Epoch: [7][100/196]	Time  3.672 ( 3.673)	Data  0.000 ( 0.000)	Loss 1.3036 (1.2817)	Acc_1  75.39 ( 75.99)	Acc_5  98.44 ( 98.55)
Test: [39/40]	Time  0.537 ( 2.297)	Loss 0.7927 (0.9860)	Adv_Loss 1.1939 (1.4916)	Acc_1  81.25 ( 74.26)	Acc_5 100.00 ( 98.24)	Adv-Acc_1  37.50 ( 44.69)	Adv-Acc_5 100.00 ( 90.92)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.08187
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [8][  0/196]	Time  3.483 ( 3.483)	Data  0.000 ( 0.000)	Loss 1.2832 (1.2832)	Acc_1  76.95 ( 76.95)	Acc_5  98.83 ( 98.83)
Epoch: [8][100/196]	Time  3.673 ( 3.673)	Data  0.000 ( 0.000)	Loss 1.2561 (1.2746)	Acc_1  81.64 ( 76.44)	Acc_5  98.83 ( 98.54)
Test: [39/40]	Time  0.534 ( 2.297)	Loss 0.7616 (0.9754)	Adv_Loss 1.1768 (1.4736)	Acc_1  87.50 ( 75.00)	Acc_5 100.00 ( 98.23)	Adv-Acc_1  43.75 ( 44.70)	Adv-Acc_5 100.00 ( 91.25)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.07679
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [9][  0/196]	Time  3.438 ( 3.438)	Data  0.000 ( 0.000)	Loss 1.2745 (1.2745)	Acc_1  76.95 ( 76.95)	Acc_5  99.61 ( 99.61)
Epoch: [9][100/196]	Time  3.670 ( 3.672)	Data  0.000 ( 0.000)	Loss 1.3108 (1.2679)	Acc_1  78.91 ( 76.86)	Acc_5  98.05 ( 98.60)
Test: [39/40]	Time  0.529 ( 2.297)	Loss 0.7587 (0.9627)	Adv_Loss 1.1620 (1.4776)	Acc_1  81.25 ( 73.81)	Acc_5 100.00 ( 98.14)	Adv-Acc_1  62.50 ( 44.76)	Adv-Acc_5 100.00 ( 90.75)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.07129
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [10][  0/196]	Time  3.480 ( 3.480)	Data  0.000 ( 0.000)	Loss 1.1832 (1.1832)	Acc_1  78.91 ( 78.91)	Acc_5  98.83 ( 98.83)
Epoch: [10][100/196]	Time  3.676 ( 3.673)	Data  0.000 ( 0.000)	Loss 1.2590 (1.2626)	Acc_1  77.73 ( 77.30)	Acc_5  98.05 ( 98.57)
Test: [39/40]	Time  0.533 ( 2.297)	Loss 0.7772 (0.9932)	Adv_Loss 1.1669 (1.4764)	Acc_1  81.25 ( 73.58)	Acc_5 100.00 ( 98.05)	Adv-Acc_1  50.00 ( 45.91)	Adv-Acc_5 100.00 ( 90.99)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.06545
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [11][  0/196]	Time  3.500 ( 3.500)	Data  0.000 ( 0.000)	Loss 1.3040 (1.3040)	Acc_1  73.83 ( 73.83)	Acc_5  97.27 ( 97.27)
Epoch: [11][100/196]	Time  3.672 ( 3.673)	Data  0.000 ( 0.000)	Loss 1.2136 (1.2554)	Acc_1  76.17 ( 77.34)	Acc_5  98.44 ( 98.55)
Test: [39/40]	Time  0.534 ( 2.299)	Loss 0.8144 (1.0100)	Adv_Loss 1.1958 (1.4970)	Acc_1  87.50 ( 72.99)	Acc_5 100.00 ( 97.93)	Adv-Acc_1  43.75 ( 44.71)	Adv-Acc_5 100.00 ( 90.18)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.05937
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [12][  0/196]	Time  3.450 ( 3.450)	Data  0.000 ( 0.000)	Loss 1.2615 (1.2615)	Acc_1  77.73 ( 77.73)	Acc_5  98.44 ( 98.44)
Epoch: [12][100/196]	Time  3.675 ( 3.676)	Data  0.000 ( 0.000)	Loss 1.2569 (1.2492)	Acc_1  77.73 ( 78.04)	Acc_5  98.83 ( 98.74)
Test: [39/40]	Time  0.532 ( 2.299)	Loss 0.7980 (0.9442)	Adv_Loss 1.2640 (1.4771)	Acc_1  87.50 ( 74.95)	Acc_5 100.00 ( 98.43)	Adv-Acc_1  56.25 ( 45.30)	Adv-Acc_5  93.75 ( 90.65)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.05314
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [13][  0/196]	Time  3.485 ( 3.485)	Data  0.000 ( 0.000)	Loss 1.2047 (1.2047)	Acc_1  78.12 ( 78.12)	Acc_5  98.83 ( 98.83)
Epoch: [13][100/196]	Time  3.680 ( 3.676)	Data  0.000 ( 0.000)	Loss 1.3241 (1.2379)	Acc_1  72.27 ( 78.16)	Acc_5  98.05 ( 98.74)
Test: [39/40]	Time  0.532 ( 2.299)	Loss 0.7987 (0.9516)	Adv_Loss 1.2365 (1.4503)	Acc_1  87.50 ( 74.60)	Acc_5 100.00 ( 98.10)	Adv-Acc_1  43.75 ( 46.62)	Adv-Acc_5 100.00 ( 91.25)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.04686
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [14][  0/196]	Time  3.443 ( 3.443)	Data  0.000 ( 0.000)	Loss 1.3050 (1.3050)	Acc_1  72.66 ( 72.66)	Acc_5  98.44 ( 98.44)
Epoch: [14][100/196]	Time  3.671 ( 3.676)	Data  0.000 ( 0.000)	Loss 1.1695 (1.2327)	Acc_1  81.64 ( 78.65)	Acc_5  99.61 ( 98.78)
Test: [39/40]	Time  0.533 ( 2.299)	Loss 0.7342 (0.9424)	Adv_Loss 1.1886 (1.4588)	Acc_1  81.25 ( 75.19)	Acc_5 100.00 ( 98.19)	Adv-Acc_1  56.25 ( 46.31)	Adv-Acc_5 100.00 ( 90.97)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.04063
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [15][  0/196]	Time  3.508 ( 3.508)	Data  0.000 ( 0.000)	Loss 1.1949 (1.1949)	Acc_1  81.64 ( 81.64)	Acc_5  99.22 ( 99.22)
Epoch: [15][100/196]	Time  3.672 ( 3.675)	Data  0.000 ( 0.000)	Loss 1.2141 (1.2288)	Acc_1  81.25 ( 78.63)	Acc_5  99.22 ( 98.83)
Test: [39/40]	Time  0.529 ( 2.298)	Loss 0.7507 (0.9254)	Adv_Loss 1.1782 (1.4507)	Acc_1  81.25 ( 75.72)	Acc_5 100.00 ( 98.32)	Adv-Acc_1  50.00 ( 46.19)	Adv-Acc_5 100.00 ( 91.19)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.03455
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [16][  0/196]	Time  3.441 ( 3.441)	Data  0.000 ( 0.000)	Loss 1.2090 (1.2090)	Acc_1  82.03 ( 82.03)	Acc_5  98.83 ( 98.83)
Epoch: [16][100/196]	Time  3.672 ( 3.673)	Data  0.000 ( 0.000)	Loss 1.3266 (1.2198)	Acc_1  77.34 ( 79.03)	Acc_5  98.83 ( 98.91)
Test: [39/40]	Time  0.531 ( 2.298)	Loss 0.7653 (0.9318)	Adv_Loss 1.2462 (1.4604)	Acc_1  87.50 ( 76.43)	Acc_5 100.00 ( 98.36)	Adv-Acc_1  43.75 ( 45.64)	Adv-Acc_5 100.00 ( 91.48)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.02871
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [17][  0/196]	Time  3.478 ( 3.478)	Data  0.000 ( 0.000)	Loss 1.1845 (1.1845)	Acc_1  79.69 ( 79.69)	Acc_5  99.22 ( 99.22)
Epoch: [17][100/196]	Time  3.674 ( 3.674)	Data  0.000 ( 0.000)	Loss 1.2355 (1.2193)	Acc_1  80.47 ( 79.35)	Acc_5  99.22 ( 98.89)
Test: [39/40]	Time  0.532 ( 2.297)	Loss 0.7534 (0.9376)	Adv_Loss 1.1772 (1.4817)	Acc_1  87.50 ( 75.07)	Acc_5 100.00 ( 98.18)	Adv-Acc_1  43.75 ( 45.19)	Adv-Acc_5  93.75 ( 90.04)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.02321
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [18][  0/196]	Time  3.481 ( 3.481)	Data  0.000 ( 0.000)	Loss 1.1854 (1.1854)	Acc_1  80.47 ( 80.47)	Acc_5  99.22 ( 99.22)
Epoch: [18][100/196]	Time  3.672 ( 3.672)	Data  0.000 ( 0.000)	Loss 1.1903 (1.2072)	Acc_1  80.86 ( 79.53)	Acc_5  99.61 ( 98.93)
Test: [39/40]	Time  0.528 ( 2.297)	Loss 0.7143 (0.9249)	Adv_Loss 1.1213 (1.4541)	Acc_1  87.50 ( 75.99)	Acc_5 100.00 ( 98.26)	Adv-Acc_1  50.00 ( 46.02)	Adv-Acc_5 100.00 ( 90.77)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.01813
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [19][  0/196]	Time  3.482 ( 3.482)	Data  0.000 ( 0.000)	Loss 1.2922 (1.2922)	Acc_1  82.81 ( 82.81)	Acc_5  98.44 ( 98.44)
Epoch: [19][100/196]	Time  3.672 ( 3.673)	Data  0.000 ( 0.000)	Loss 1.2128 (1.2033)	Acc_1  80.86 ( 79.96)	Acc_5  98.05 ( 98.96)
Test: [39/40]	Time  0.528 ( 2.297)	Loss 0.7505 (0.9302)	Adv_Loss 1.1984 (1.4329)	Acc_1  87.50 ( 76.46)	Acc_5 100.00 ( 98.38)	Adv-Acc_1  50.00 ( 47.12)	Adv-Acc_5 100.00 ( 91.90)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.01355
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [20][  0/196]	Time  3.481 ( 3.481)	Data  0.000 ( 0.000)	Loss 1.2175 (1.2175)	Acc_1  80.08 ( 80.08)	Acc_5  99.22 ( 99.22)
Epoch: [20][100/196]	Time  3.671 ( 3.673)	Data  0.000 ( 0.000)	Loss 1.1085 (1.2017)	Acc_1  82.81 ( 80.28)	Acc_5  99.61 ( 98.91)
Test: [39/40]	Time  0.531 ( 2.296)	Loss 0.7769 (0.9220)	Adv_Loss 1.2458 (1.4458)	Acc_1  81.25 ( 76.26)	Acc_5 100.00 ( 98.52)	Adv-Acc_1  50.00 ( 46.28)	Adv-Acc_5 100.00 ( 91.54)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.00955
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [21][  0/196]	Time  3.446 ( 3.446)	Data  0.000 ( 0.000)	Loss 1.1945 (1.1945)	Acc_1  77.73 ( 77.73)	Acc_5  98.44 ( 98.44)
Epoch: [21][100/196]	Time  3.674 ( 3.673)	Data  0.000 ( 0.000)	Loss 1.2008 (1.1929)	Acc_1  81.64 ( 80.19)	Acc_5  98.05 ( 99.01)
Test: [39/40]	Time  0.529 ( 2.297)	Loss 0.7755 (0.9272)	Adv_Loss 1.2302 (1.4352)	Acc_1  81.25 ( 75.91)	Acc_5 100.00 ( 98.40)	Adv-Acc_1  50.00 ( 47.33)	Adv-Acc_5  87.50 ( 91.91)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.00618
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [22][  0/196]	Time  3.490 ( 3.490)	Data  0.000 ( 0.000)	Loss 1.1600 (1.1600)	Acc_1  79.30 ( 79.30)	Acc_5  99.61 ( 99.61)
Epoch: [22][100/196]	Time  3.669 ( 3.673)	Data  0.000 ( 0.000)	Loss 1.2470 (1.1912)	Acc_1  78.52 ( 80.54)	Acc_5  99.22 ( 99.11)
Test: [39/40]	Time  0.528 ( 2.296)	Loss 0.7880 (0.9245)	Adv_Loss 1.2425 (1.4316)	Acc_1  87.50 ( 76.48)	Acc_5 100.00 ( 98.44)	Adv-Acc_1  43.75 ( 46.82)	Adv-Acc_5  93.75 ( 92.01)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.00351
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [23][  0/196]	Time  3.504 ( 3.504)	Data  0.000 ( 0.000)	Loss 1.1871 (1.1871)	Acc_1  80.86 ( 80.86)	Acc_5  98.83 ( 98.83)
Epoch: [23][100/196]	Time  3.677 ( 3.677)	Data  0.000 ( 0.000)	Loss 1.1710 (1.1846)	Acc_1  83.20 ( 80.93)	Acc_5  99.22 ( 99.12)
Test: [39/40]	Time  0.533 ( 2.300)	Loss 0.7731 (0.9247)	Adv_Loss 1.2424 (1.4501)	Acc_1  87.50 ( 76.23)	Acc_5 100.00 ( 98.36)	Adv-Acc_1  37.50 ( 45.15)	Adv-Acc_5  93.75 ( 91.63)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.00157
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [24][  0/196]	Time  3.494 ( 3.494)	Data  0.000 ( 0.000)	Loss 1.1344 (1.1344)	Acc_1  81.64 ( 81.64)	Acc_5  99.61 ( 99.61)
Epoch: [24][100/196]	Time  3.677 ( 3.676)	Data  0.000 ( 0.000)	Loss 1.2054 (1.1837)	Acc_1  78.91 ( 80.70)	Acc_5  98.83 ( 99.09)
Test: [39/40]	Time  0.531 ( 2.299)	Loss 0.7820 (0.9173)	Adv_Loss 1.2539 (1.4298)	Acc_1  81.25 ( 76.63)	Acc_5 100.00 ( 98.40)	Adv-Acc_1  43.75 ( 47.45)	Adv-Acc_5  93.75 ( 92.04)
conv1 SubnetConv(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.98842592592592
layer1.0.conv1 SubnetConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.99837239583333
layer1.0.conv2 SubnetConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.99837239583333
layer1.1.conv1 SubnetConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.99837239583333
layer1.1.conv2 SubnetConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.99837239583333
layer2.0.conv1 SubnetConv(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) 89.99972873263889
layer2.0.conv2 SubnetConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.99972873263889
layer2.0.shortcut.0 SubnetConv(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) 89.990234375
layer2.1.conv1 SubnetConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.99972873263889
layer2.1.conv2 SubnetConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.99972873263889
layer3.0.conv1 SubnetConv(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) 89.99972873263889
layer3.0.conv2 SubnetConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.99989827473958
layer3.0.shortcut.0 SubnetConv(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) 89.9993896484375
layer3.1.conv1 SubnetConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.99989827473958
layer3.1.conv2 SubnetConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.99989827473958
layer4.0.conv1 SubnetConv(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) 89.99998304578993
layer4.0.conv2 SubnetConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.99998304578993
layer4.0.shortcut.0 SubnetConv(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) 89.9993896484375
layer4.1.conv1 SubnetConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.99998304578993
layer4.1.conv2 SubnetConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.99998304578993
linear SubnetLinear(in_features=512, out_features=10, bias=True) 90.0
=> Reading YAML config from configs/configs.yml
#################### Fine-tuning network ####################
===>>  gradient for importance_scores: None  | fine-tuning important weigths only
Initialization relevance score with None initialization
Files already downloaded and verified
Files already downloaded and verified
Traing loader: 50000 images, Test loader: 10000 images
Test: [39/40]	Time  0.530 ( 2.304)	Loss 0.7820 (0.9173)	Adv_Loss 1.2525 (1.4298)	Acc_1  81.25 ( 76.63)	Acc_5 100.00 ( 98.40)	Adv-Acc_1  43.75 ( 47.54)	Adv-Acc_5  93.75 ( 92.05)
variable = conv1.weight, Gradient requires_grad = True
variable = conv1.popup_scores, Gradient requires_grad = False
variable = bn1.weight, Gradient requires_grad = True
variable = bn1.bias, Gradient requires_grad = True
variable = layer1.0.conv1.weight, Gradient requires_grad = True
variable = layer1.0.conv1.popup_scores, Gradient requires_grad = False
variable = layer1.0.bn1.weight, Gradient requires_grad = True
variable = layer1.0.bn1.bias, Gradient requires_grad = True
variable = layer1.0.conv2.weight, Gradient requires_grad = True
variable = layer1.0.conv2.popup_scores, Gradient requires_grad = False
variable = layer1.0.bn2.weight, Gradient requires_grad = True
variable = layer1.0.bn2.bias, Gradient requires_grad = True
variable = layer1.1.conv1.weight, Gradient requires_grad = True
variable = layer1.1.conv1.popup_scores, Gradient requires_grad = False
variable = layer1.1.bn1.weight, Gradient requires_grad = True
variable = layer1.1.bn1.bias, Gradient requires_grad = True
variable = layer1.1.conv2.weight, Gradient requires_grad = True
variable = layer1.1.conv2.popup_scores, Gradient requires_grad = False
variable = layer1.1.bn2.weight, Gradient requires_grad = True
variable = layer1.1.bn2.bias, Gradient requires_grad = True
variable = layer2.0.conv1.weight, Gradient requires_grad = True
variable = layer2.0.conv1.popup_scores, Gradient requires_grad = False
variable = layer2.0.bn1.weight, Gradient requires_grad = True
variable = layer2.0.bn1.bias, Gradient requires_grad = True
variable = layer2.0.conv2.weight, Gradient requires_grad = True
variable = layer2.0.conv2.popup_scores, Gradient requires_grad = False
variable = layer2.0.bn2.weight, Gradient requires_grad = True
variable = layer2.0.bn2.bias, Gradient requires_grad = True
variable = layer2.0.shortcut.0.weight, Gradient requires_grad = True
variable = layer2.0.shortcut.0.popup_scores, Gradient requires_grad = False
variable = layer2.0.shortcut.1.weight, Gradient requires_grad = True
variable = layer2.0.shortcut.1.bias, Gradient requires_grad = True
variable = layer2.1.conv1.weight, Gradient requires_grad = True
variable = layer2.1.conv1.popup_scores, Gradient requires_grad = False
variable = layer2.1.bn1.weight, Gradient requires_grad = True
variable = layer2.1.bn1.bias, Gradient requires_grad = True
variable = layer2.1.conv2.weight, Gradient requires_grad = True
variable = layer2.1.conv2.popup_scores, Gradient requires_grad = False
variable = layer2.1.bn2.weight, Gradient requires_grad = True
variable = layer2.1.bn2.bias, Gradient requires_grad = True
variable = layer3.0.conv1.weight, Gradient requires_grad = True
variable = layer3.0.conv1.popup_scores, Gradient requires_grad = False
variable = layer3.0.bn1.weight, Gradient requires_grad = True
variable = layer3.0.bn1.bias, Gradient requires_grad = True
variable = layer3.0.conv2.weight, Gradient requires_grad = True
variable = layer3.0.conv2.popup_scores, Gradient requires_grad = False
variable = layer3.0.bn2.weight, Gradient requires_grad = True
variable = layer3.0.bn2.bias, Gradient requires_grad = True
variable = layer3.0.shortcut.0.weight, Gradient requires_grad = True
variable = layer3.0.shortcut.0.popup_scores, Gradient requires_grad = False
variable = layer3.0.shortcut.1.weight, Gradient requires_grad = True
variable = layer3.0.shortcut.1.bias, Gradient requires_grad = True
variable = layer3.1.conv1.weight, Gradient requires_grad = True
variable = layer3.1.conv1.popup_scores, Gradient requires_grad = False
variable = layer3.1.bn1.weight, Gradient requires_grad = True
variable = layer3.1.bn1.bias, Gradient requires_grad = True
variable = layer3.1.conv2.weight, Gradient requires_grad = True
variable = layer3.1.conv2.popup_scores, Gradient requires_grad = False
variable = layer3.1.bn2.weight, Gradient requires_grad = True
variable = layer3.1.bn2.bias, Gradient requires_grad = True
variable = layer4.0.conv1.weight, Gradient requires_grad = True
variable = layer4.0.conv1.popup_scores, Gradient requires_grad = False
variable = layer4.0.bn1.weight, Gradient requires_grad = True
variable = layer4.0.bn1.bias, Gradient requires_grad = True
variable = layer4.0.conv2.weight, Gradient requires_grad = True
variable = layer4.0.conv2.popup_scores, Gradient requires_grad = False
variable = layer4.0.bn2.weight, Gradient requires_grad = True
variable = layer4.0.bn2.bias, Gradient requires_grad = True
variable = layer4.0.shortcut.0.weight, Gradient requires_grad = True
variable = layer4.0.shortcut.0.popup_scores, Gradient requires_grad = False
variable = layer4.0.shortcut.1.weight, Gradient requires_grad = True
variable = layer4.0.shortcut.1.bias, Gradient requires_grad = True
variable = layer4.1.conv1.weight, Gradient requires_grad = True
variable = layer4.1.conv1.popup_scores, Gradient requires_grad = False
variable = layer4.1.bn1.weight, Gradient requires_grad = True
variable = layer4.1.bn1.bias, Gradient requires_grad = True
variable = layer4.1.conv2.weight, Gradient requires_grad = True
variable = layer4.1.conv2.popup_scores, Gradient requires_grad = False
variable = layer4.1.bn2.weight, Gradient requires_grad = True
variable = layer4.1.bn2.bias, Gradient requires_grad = True
variable = linear.weight, Gradient requires_grad = True
variable = linear.bias, Gradient requires_grad = True
variable = linear.popup_scores, Gradient requires_grad = False
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.00996
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [0][  0/196]	Time  3.497 ( 3.497)	Data  0.000 ( 0.000)	Loss 1.1698 (1.1698)	Acc_1  82.03 ( 82.03)	Acc_5  99.61 ( 99.61)
Epoch: [0][100/196]	Time  3.680 ( 3.677)	Data  0.000 ( 0.000)	Loss 1.2131 (1.1884)	Acc_1  80.08 ( 81.08)	Acc_5  99.22 ( 99.13)
Test: [39/40]	Time  0.539 ( 2.294)	Loss 0.7538 (0.9153)	Adv_Loss 1.1789 (1.4066)	Acc_1  87.50 ( 76.71)	Acc_5 100.00 ( 98.52)	Adv-Acc_1  50.00 ( 48.54)	Adv-Acc_5 100.00 ( 91.93)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.01000
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [1][  0/196]	Time  3.496 ( 3.496)	Data  0.000 ( 0.000)	Loss 1.1567 (1.1567)	Acc_1  79.69 ( 79.69)	Acc_5  99.22 ( 99.22)
Epoch: [1][100/196]	Time  3.677 ( 3.678)	Data  0.000 ( 0.000)	Loss 1.1215 (1.1791)	Acc_1  85.55 ( 81.38)	Acc_5  98.44 ( 99.10)
Test: [39/40]	Time  0.526 ( 2.294)	Loss 0.7647 (0.9148)	Adv_Loss 1.1848 (1.4046)	Acc_1  87.50 ( 76.05)	Acc_5 100.00 ( 98.49)	Adv-Acc_1  37.50 ( 48.34)	Adv-Acc_5 100.00 ( 92.19)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.00996
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [2][  0/196]	Time  3.569 ( 3.569)	Data  0.000 ( 0.000)	Loss 1.2127 (1.2127)	Acc_1  83.20 ( 83.20)	Acc_5  99.22 ( 99.22)
Epoch: [2][100/196]	Time  3.672 ( 3.675)	Data  0.000 ( 0.000)	Loss 1.1873 (1.1807)	Acc_1  76.95 ( 80.79)	Acc_5  98.83 ( 99.06)
Test: [39/40]	Time  0.529 ( 2.291)	Loss 0.7305 (0.9046)	Adv_Loss 1.1435 (1.4015)	Acc_1  87.50 ( 76.76)	Acc_5 100.00 ( 98.57)	Adv-Acc_1  43.75 ( 47.99)	Adv-Acc_5 100.00 ( 91.95)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.00984
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [3][  0/196]	Time  3.414 ( 3.414)	Data  0.000 ( 0.000)	Loss 1.0905 (1.0905)	Acc_1  84.38 ( 84.38)	Acc_5  99.22 ( 99.22)
Epoch: [3][100/196]	Time  3.672 ( 3.672)	Data  0.000 ( 0.000)	Loss 1.1623 (1.1799)	Acc_1  84.77 ( 81.28)	Acc_5  98.44 ( 99.03)
Test: [39/40]	Time  0.543 ( 2.309)	Loss 0.7540 (0.9108)	Adv_Loss 1.1910 (1.4063)	Acc_1  87.50 ( 76.93)	Acc_5 100.00 ( 98.53)	Adv-Acc_1  43.75 ( 48.19)	Adv-Acc_5  93.75 ( 92.13)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.00965
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [4][  0/196]	Time  3.482 ( 3.482)	Data  0.000 ( 0.000)	Loss 1.1564 (1.1564)	Acc_1  83.59 ( 83.59)	Acc_5  99.22 ( 99.22)
Epoch: [4][100/196]	Time  3.674 ( 3.678)	Data  0.000 ( 0.000)	Loss 1.2127 (1.1757)	Acc_1  76.56 ( 81.20)	Acc_5  98.05 ( 99.09)
Test: [39/40]	Time  0.532 ( 2.294)	Loss 0.7628 (0.9104)	Adv_Loss 1.1924 (1.4058)	Acc_1  87.50 ( 76.25)	Acc_5 100.00 ( 98.50)	Adv-Acc_1  43.75 ( 48.11)	Adv-Acc_5 100.00 ( 92.40)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.00938
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [5][  0/196]	Time  3.425 ( 3.425)	Data  0.000 ( 0.000)	Loss 1.1144 (1.1144)	Acc_1  84.77 ( 84.77)	Acc_5  99.61 ( 99.61)
Epoch: [5][100/196]	Time  3.673 ( 3.676)	Data  0.000 ( 0.000)	Loss 1.2162 (1.1665)	Acc_1  80.47 ( 81.66)	Acc_5  98.44 ( 99.16)
Test: [39/40]	Time  0.536 ( 2.292)	Loss 0.7351 (0.8974)	Adv_Loss 1.1650 (1.4015)	Acc_1  87.50 ( 77.02)	Acc_5 100.00 ( 98.50)	Adv-Acc_1  43.75 ( 48.46)	Adv-Acc_5 100.00 ( 92.07)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.00905
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [6][  0/196]	Time  3.476 ( 3.476)	Data  0.000 ( 0.000)	Loss 1.1121 (1.1121)	Acc_1  84.77 ( 84.77)	Acc_5  98.44 ( 98.44)
Epoch: [6][100/196]	Time  3.677 ( 3.674)	Data  0.000 ( 0.000)	Loss 1.2279 (1.1700)	Acc_1  79.69 ( 81.70)	Acc_5  98.83 ( 99.15)
Test: [39/40]	Time  0.533 ( 2.296)	Loss 0.7421 (0.8990)	Adv_Loss 1.1702 (1.4002)	Acc_1  87.50 ( 76.96)	Acc_5 100.00 ( 98.60)	Adv-Acc_1  43.75 ( 48.00)	Adv-Acc_5 100.00 ( 92.31)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.00864
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [7][  0/196]	Time  3.425 ( 3.425)	Data  0.000 ( 0.000)	Loss 1.1432 (1.1432)	Acc_1  80.47 ( 80.47)	Acc_5  99.61 ( 99.61)
Epoch: [7][100/196]	Time  3.682 ( 3.678)	Data  0.000 ( 0.000)	Loss 1.1962 (1.1673)	Acc_1  80.08 ( 81.64)	Acc_5  99.61 ( 99.18)
Test: [39/40]	Time  0.527 ( 2.293)	Loss 0.7224 (0.8958)	Adv_Loss 1.1365 (1.4005)	Acc_1  87.50 ( 76.77)	Acc_5 100.00 ( 98.50)	Adv-Acc_1  56.25 ( 48.11)	Adv-Acc_5 100.00 ( 91.84)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.00819
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [8][  0/196]	Time  3.465 ( 3.465)	Data  0.000 ( 0.000)	Loss 1.1795 (1.1795)	Acc_1  78.91 ( 78.91)	Acc_5  98.83 ( 98.83)
Epoch: [8][100/196]	Time  3.672 ( 3.675)	Data  0.000 ( 0.000)	Loss 1.1523 (1.1722)	Acc_1  86.72 ( 81.46)	Acc_5  99.22 ( 99.08)
Test: [39/40]	Time  0.533 ( 2.293)	Loss 0.7496 (0.9037)	Adv_Loss 1.1862 (1.4011)	Acc_1  87.50 ( 76.77)	Acc_5 100.00 ( 98.49)	Adv-Acc_1  43.75 ( 48.16)	Adv-Acc_5  93.75 ( 92.27)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.00768
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [9][  0/196]	Time  3.432 ( 3.432)	Data  0.000 ( 0.000)	Loss 1.1986 (1.1986)	Acc_1  83.20 ( 83.20)	Acc_5  99.61 ( 99.61)
Epoch: [9][100/196]	Time  3.673 ( 3.674)	Data  0.000 ( 0.000)	Loss 1.1981 (1.1663)	Acc_1  81.25 ( 81.89)	Acc_5  98.83 ( 99.23)
Test: [39/40]	Time  0.523 ( 2.291)	Loss 0.7294 (0.8955)	Adv_Loss 1.1483 (1.3986)	Acc_1  87.50 ( 76.33)	Acc_5 100.00 ( 98.57)	Adv-Acc_1  43.75 ( 48.21)	Adv-Acc_5 100.00 ( 92.14)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.00713
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [10][  0/196]	Time  3.473 ( 3.473)	Data  0.000 ( 0.000)	Loss 1.0849 (1.0849)	Acc_1  83.20 ( 83.20)	Acc_5 100.00 (100.00)
Epoch: [10][100/196]	Time  3.675 ( 3.676)	Data  0.000 ( 0.000)	Loss 1.1847 (1.1693)	Acc_1  80.47 ( 81.74)	Acc_5  98.44 ( 99.08)
Test: [39/40]	Time  0.527 ( 2.294)	Loss 0.7376 (0.9067)	Adv_Loss 1.1479 (1.4016)	Acc_1  87.50 ( 76.37)	Acc_5 100.00 ( 98.57)	Adv-Acc_1  43.75 ( 48.27)	Adv-Acc_5 100.00 ( 92.04)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.00655
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [11][  0/196]	Time  3.466 ( 3.466)	Data  0.000 ( 0.000)	Loss 1.2123 (1.2123)	Acc_1  78.52 ( 78.52)	Acc_5  98.83 ( 98.83)
Epoch: [11][100/196]	Time  3.676 ( 3.676)	Data  0.000 ( 0.000)	Loss 1.1240 (1.1679)	Acc_1  82.81 ( 81.60)	Acc_5  98.05 ( 99.04)
Test: [39/40]	Time  0.526 ( 2.293)	Loss 0.7424 (0.9008)	Adv_Loss 1.1735 (1.4037)	Acc_1  87.50 ( 76.43)	Acc_5 100.00 ( 98.44)	Adv-Acc_1  43.75 ( 48.19)	Adv-Acc_5  93.75 ( 92.06)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.00594
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [12][  0/196]	Time  3.490 ( 3.490)	Data  0.000 ( 0.000)	Loss 1.2140 (1.2140)	Acc_1  82.42 ( 82.42)	Acc_5  98.44 ( 98.44)
Epoch: [12][100/196]	Time  3.673 ( 3.677)	Data  0.000 ( 0.000)	Loss 1.1807 (1.1666)	Acc_1  80.08 ( 81.56)	Acc_5  98.83 ( 99.21)
Test: [39/40]	Time  0.529 ( 2.295)	Loss 0.7348 (0.8950)	Adv_Loss 1.1578 (1.3995)	Acc_1  87.50 ( 76.81)	Acc_5 100.00 ( 98.43)	Adv-Acc_1  50.00 ( 48.44)	Adv-Acc_5 100.00 ( 92.13)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.00531
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [13][  0/196]	Time  3.471 ( 3.471)	Data  0.000 ( 0.000)	Loss 1.1624 (1.1624)	Acc_1  82.03 ( 82.03)	Acc_5  99.22 ( 99.22)
Epoch: [13][100/196]	Time  3.678 ( 3.677)	Data  0.000 ( 0.000)	Loss 1.2528 (1.1618)	Acc_1  76.56 ( 81.96)	Acc_5  98.44 ( 99.17)
Test: [39/40]	Time  0.528 ( 2.294)	Loss 0.7397 (0.8957)	Adv_Loss 1.1670 (1.3959)	Acc_1  87.50 ( 76.66)	Acc_5 100.00 ( 98.57)	Adv-Acc_1  43.75 ( 48.68)	Adv-Acc_5 100.00 ( 92.04)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.00469
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [14][  0/196]	Time  3.438 ( 3.438)	Data  0.000 ( 0.000)	Loss 1.2489 (1.2489)	Acc_1  76.17 ( 76.17)	Acc_5  98.05 ( 98.05)
Epoch: [14][100/196]	Time  3.672 ( 3.676)	Data  0.000 ( 0.000)	Loss 1.0825 (1.1617)	Acc_1  83.59 ( 81.95)	Acc_5  99.22 ( 99.18)
Test: [39/40]	Time  0.529 ( 2.295)	Loss 0.7517 (0.8969)	Adv_Loss 1.1930 (1.4006)	Acc_1  87.50 ( 76.48)	Acc_5 100.00 ( 98.46)	Adv-Acc_1  43.75 ( 48.60)	Adv-Acc_5 100.00 ( 91.79)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.00406
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [15][  0/196]	Time  3.465 ( 3.465)	Data  0.000 ( 0.000)	Loss 1.1366 (1.1366)	Acc_1  84.38 ( 84.38)	Acc_5 100.00 (100.00)
Epoch: [15][100/196]	Time  3.676 ( 3.676)	Data  0.000 ( 0.000)	Loss 1.1140 (1.1623)	Acc_1  85.55 ( 82.09)	Acc_5  99.22 ( 99.21)
Test: [39/40]	Time  0.526 ( 2.294)	Loss 0.7252 (0.8936)	Adv_Loss 1.1545 (1.3995)	Acc_1  87.50 ( 76.86)	Acc_5 100.00 ( 98.58)	Adv-Acc_1  43.75 ( 48.18)	Adv-Acc_5 100.00 ( 92.04)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.00345
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [16][  0/196]	Time  3.434 ( 3.434)	Data  0.000 ( 0.000)	Loss 1.1472 (1.1472)	Acc_1  85.55 ( 85.55)	Acc_5  98.83 ( 98.83)
Epoch: [16][100/196]	Time  3.678 ( 3.676)	Data  0.000 ( 0.000)	Loss 1.2538 (1.1592)	Acc_1  81.25 ( 82.07)	Acc_5  99.22 ( 99.24)
Test: [39/40]	Time  0.538 ( 2.299)	Loss 0.7289 (0.8955)	Adv_Loss 1.1565 (1.3975)	Acc_1  87.50 ( 76.71)	Acc_5 100.00 ( 98.57)	Adv-Acc_1  43.75 ( 48.43)	Adv-Acc_5 100.00 ( 92.13)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.00287
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [17][  0/196]	Time  3.482 ( 3.482)	Data  0.000 ( 0.000)	Loss 1.1437 (1.1437)	Acc_1  81.25 ( 81.25)	Acc_5  98.83 ( 98.83)
Epoch: [17][100/196]	Time  3.689 ( 3.694)	Data  0.000 ( 0.000)	Loss 1.1861 (1.1655)	Acc_1  82.03 ( 81.94)	Acc_5  99.61 ( 99.12)
Test: [39/40]	Time  0.526 ( 2.293)	Loss 0.7113 (0.8826)	Adv_Loss 1.1404 (1.3959)	Acc_1  87.50 ( 76.65)	Acc_5 100.00 ( 98.58)	Adv-Acc_1  43.75 ( 48.05)	Adv-Acc_5 100.00 ( 92.01)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.00232
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [18][  0/196]	Time  3.435 ( 3.435)	Data  0.000 ( 0.000)	Loss 1.1633 (1.1633)	Acc_1  81.64 ( 81.64)	Acc_5  99.61 ( 99.61)
Epoch: [18][100/196]	Time  3.697 ( 3.684)	Data  0.000 ( 0.000)	Loss 1.1351 (1.1592)	Acc_1  83.98 ( 82.13)	Acc_5  99.22 ( 99.17)
Test: [39/40]	Time  0.562 ( 2.321)	Loss 0.7249 (0.8911)	Adv_Loss 1.1511 (1.3959)	Acc_1  87.50 ( 76.53)	Acc_5 100.00 ( 98.58)	Adv-Acc_1  43.75 ( 48.54)	Adv-Acc_5 100.00 ( 91.77)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.00181
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [19][  0/196]	Time  3.532 ( 3.532)	Data  0.000 ( 0.000)	Loss 1.2518 (1.2518)	Acc_1  83.20 ( 83.20)	Acc_5  98.83 ( 98.83)
Epoch: [19][100/196]	Time  3.677 ( 3.700)	Data  0.000 ( 0.000)	Loss 1.1603 (1.1622)	Acc_1  82.81 ( 81.95)	Acc_5  99.22 ( 99.21)
Test: [39/40]	Time  0.529 ( 2.294)	Loss 0.7602 (0.9051)	Adv_Loss 1.2120 (1.4043)	Acc_1  87.50 ( 76.84)	Acc_5 100.00 ( 98.47)	Adv-Acc_1  50.00 ( 48.33)	Adv-Acc_5  93.75 ( 92.03)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.00136
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [20][  0/196]	Time  3.478 ( 3.478)	Data  0.000 ( 0.000)	Loss 1.1669 (1.1669)	Acc_1  81.64 ( 81.64)	Acc_5  99.61 ( 99.61)
Epoch: [20][100/196]	Time  3.712 ( 3.683)	Data  0.000 ( 0.000)	Loss 1.0688 (1.1658)	Acc_1  84.38 ( 81.81)	Acc_5  99.61 ( 99.10)
Test: [39/40]	Time  0.529 ( 2.294)	Loss 0.7289 (0.8930)	Adv_Loss 1.1575 (1.3953)	Acc_1  87.50 ( 76.79)	Acc_5 100.00 ( 98.63)	Adv-Acc_1  43.75 ( 48.35)	Adv-Acc_5 100.00 ( 92.29)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.00095
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [21][  0/196]	Time  3.423 ( 3.423)	Data  0.000 ( 0.000)	Loss 1.1562 (1.1562)	Acc_1  79.30 ( 79.30)	Acc_5  98.05 ( 98.05)
Epoch: [21][100/196]	Time  3.684 ( 3.678)	Data  0.000 ( 0.000)	Loss 1.1757 (1.1617)	Acc_1  82.03 ( 81.86)	Acc_5  99.22 ( 99.18)
Test: [39/40]	Time  0.560 ( 2.315)	Loss 0.7585 (0.9050)	Adv_Loss 1.2001 (1.3994)	Acc_1  87.50 ( 76.82)	Acc_5 100.00 ( 98.49)	Adv-Acc_1  43.75 ( 48.63)	Adv-Acc_5  93.75 ( 92.31)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.00062
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [22][  0/196]	Time  3.554 ( 3.554)	Data  0.000 ( 0.000)	Loss 1.1347 (1.1347)	Acc_1  80.08 ( 80.08)	Acc_5  99.61 ( 99.61)
Epoch: [22][100/196]	Time  3.711 ( 3.714)	Data  0.000 ( 0.000)	Loss 1.2218 (1.1660)	Acc_1  80.47 ( 81.69)	Acc_5  98.83 ( 99.23)
Test: [39/40]	Time  0.527 ( 2.296)	Loss 0.7262 (0.8916)	Adv_Loss 1.1471 (1.3949)	Acc_1  87.50 ( 76.75)	Acc_5 100.00 ( 98.56)	Adv-Acc_1  43.75 ( 48.47)	Adv-Acc_5 100.00 ( 92.08)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.00035
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [23][  0/196]	Time  3.438 ( 3.438)	Data  0.000 ( 0.000)	Loss 1.1771 (1.1771)	Acc_1  81.64 ( 81.64)	Acc_5  98.83 ( 98.83)
Epoch: [23][100/196]	Time  3.705 ( 3.698)	Data  0.000 ( 0.000)	Loss 1.1563 (1.1642)	Acc_1  83.59 ( 81.85)	Acc_5  99.22 ( 99.23)
Test: [39/40]	Time  0.558 ( 2.316)	Loss 0.7511 (0.9007)	Adv_Loss 1.1784 (1.3960)	Acc_1  87.50 ( 76.83)	Acc_5 100.00 ( 98.59)	Adv-Acc_1  43.75 ( 48.48)	Adv-Acc_5 100.00 ( 92.47)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.00016
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [24][  0/196]	Time  3.550 ( 3.550)	Data  0.000 ( 0.000)	Loss 1.1556 (1.1556)	Acc_1  79.69 ( 79.69)	Acc_5  99.22 ( 99.22)
Epoch: [24][100/196]	Time  3.699 ( 3.692)	Data  0.000 ( 0.000)	Loss 1.1877 (1.1695)	Acc_1  80.47 ( 81.49)	Acc_5  99.22 ( 99.12)
Test: [39/40]	Time  0.537 ( 2.317)	Loss 0.7367 (0.8960)	Adv_Loss 1.1591 (1.3973)	Acc_1  87.50 ( 76.63)	Acc_5 100.00 ( 98.60)	Adv-Acc_1  43.75 ( 48.60)	Adv-Acc_5  93.75 ( 92.29)
conv1 SubnetConv(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.98842592592592
layer1.0.conv1 SubnetConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.99837239583333
layer1.0.conv2 SubnetConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.99837239583333
layer1.1.conv1 SubnetConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.99837239583333
layer1.1.conv2 SubnetConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.99837239583333
layer2.0.conv1 SubnetConv(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) 89.99972873263889
layer2.0.conv2 SubnetConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.99972873263889
layer2.0.shortcut.0 SubnetConv(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) 89.990234375
layer2.1.conv1 SubnetConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.99972873263889
layer2.1.conv2 SubnetConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.99972873263889
layer3.0.conv1 SubnetConv(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) 89.99972873263889
layer3.0.conv2 SubnetConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.99989827473958
layer3.0.shortcut.0 SubnetConv(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) 89.9993896484375
layer3.1.conv1 SubnetConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.99989827473958
layer3.1.conv2 SubnetConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.99989827473958
layer4.0.conv1 SubnetConv(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) 89.99998304578993
layer4.0.conv2 SubnetConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.99998304578993
layer4.0.shortcut.0 SubnetConv(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) 89.9993896484375
layer4.1.conv1 SubnetConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.99998304578993
layer4.1.conv2 SubnetConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.99998304578993
linear SubnetLinear(in_features=512, out_features=10, bias=True) 90.0

=> Reading YAML config from configs/configs.yml
#################### Pruning network ####################
===>>  gradient for weights: None  | training importance scores only
Initialization relevance score with None initialization
Files already downloaded and verified
Files already downloaded and verified
Traing loader: 50000 images, Test loader: 10000 images
Initialization relevance score proportional to weight magnitudes (OVERWRITING SOURCE NET SCORES)
Test: [39/40]	Time  0.543 ( 2.304)	Loss 2.6512 (2.4906)	Adv_Loss 2.6823 (2.5221)	Acc_1  18.75 ( 14.21)	Acc_5  56.25 ( 50.00)	Adv-Acc_1  18.75 ( 12.42)	Adv-Acc_5  56.25 ( 50.00)
variable = conv1.weight, Gradient requires_grad = False
variable = conv1.popup_scores, Gradient requires_grad = True
variable = bn1.weight, Gradient requires_grad = True
variable = bn1.bias, Gradient requires_grad = True
variable = layer1.0.conv1.weight, Gradient requires_grad = False
variable = layer1.0.conv1.popup_scores, Gradient requires_grad = True
variable = layer1.0.bn1.weight, Gradient requires_grad = True
variable = layer1.0.bn1.bias, Gradient requires_grad = True
variable = layer1.0.conv2.weight, Gradient requires_grad = False
variable = layer1.0.conv2.popup_scores, Gradient requires_grad = True
variable = layer1.0.bn2.weight, Gradient requires_grad = True
variable = layer1.0.bn2.bias, Gradient requires_grad = True
variable = layer1.1.conv1.weight, Gradient requires_grad = False
variable = layer1.1.conv1.popup_scores, Gradient requires_grad = True
variable = layer1.1.bn1.weight, Gradient requires_grad = True
variable = layer1.1.bn1.bias, Gradient requires_grad = True
variable = layer1.1.conv2.weight, Gradient requires_grad = False
variable = layer1.1.conv2.popup_scores, Gradient requires_grad = True
variable = layer1.1.bn2.weight, Gradient requires_grad = True
variable = layer1.1.bn2.bias, Gradient requires_grad = True
variable = layer2.0.conv1.weight, Gradient requires_grad = False
variable = layer2.0.conv1.popup_scores, Gradient requires_grad = True
variable = layer2.0.bn1.weight, Gradient requires_grad = True
variable = layer2.0.bn1.bias, Gradient requires_grad = True
variable = layer2.0.conv2.weight, Gradient requires_grad = False
variable = layer2.0.conv2.popup_scores, Gradient requires_grad = True
variable = layer2.0.bn2.weight, Gradient requires_grad = True
variable = layer2.0.bn2.bias, Gradient requires_grad = True
variable = layer2.0.shortcut.0.weight, Gradient requires_grad = False
variable = layer2.0.shortcut.0.popup_scores, Gradient requires_grad = True
variable = layer2.0.shortcut.1.weight, Gradient requires_grad = True
variable = layer2.0.shortcut.1.bias, Gradient requires_grad = True
variable = layer2.1.conv1.weight, Gradient requires_grad = False
variable = layer2.1.conv1.popup_scores, Gradient requires_grad = True
variable = layer2.1.bn1.weight, Gradient requires_grad = True
variable = layer2.1.bn1.bias, Gradient requires_grad = True
variable = layer2.1.conv2.weight, Gradient requires_grad = False
variable = layer2.1.conv2.popup_scores, Gradient requires_grad = True
variable = layer2.1.bn2.weight, Gradient requires_grad = True
variable = layer2.1.bn2.bias, Gradient requires_grad = True
variable = layer3.0.conv1.weight, Gradient requires_grad = False
variable = layer3.0.conv1.popup_scores, Gradient requires_grad = True
variable = layer3.0.bn1.weight, Gradient requires_grad = True
variable = layer3.0.bn1.bias, Gradient requires_grad = True
variable = layer3.0.conv2.weight, Gradient requires_grad = False
variable = layer3.0.conv2.popup_scores, Gradient requires_grad = True
variable = layer3.0.bn2.weight, Gradient requires_grad = True
variable = layer3.0.bn2.bias, Gradient requires_grad = True
variable = layer3.0.shortcut.0.weight, Gradient requires_grad = False
variable = layer3.0.shortcut.0.popup_scores, Gradient requires_grad = True
variable = layer3.0.shortcut.1.weight, Gradient requires_grad = True
variable = layer3.0.shortcut.1.bias, Gradient requires_grad = True
variable = layer3.1.conv1.weight, Gradient requires_grad = False
variable = layer3.1.conv1.popup_scores, Gradient requires_grad = True
variable = layer3.1.bn1.weight, Gradient requires_grad = True
variable = layer3.1.bn1.bias, Gradient requires_grad = True
variable = layer3.1.conv2.weight, Gradient requires_grad = False
variable = layer3.1.conv2.popup_scores, Gradient requires_grad = True
variable = layer3.1.bn2.weight, Gradient requires_grad = True
variable = layer3.1.bn2.bias, Gradient requires_grad = True
variable = layer4.0.conv1.weight, Gradient requires_grad = False
variable = layer4.0.conv1.popup_scores, Gradient requires_grad = True
variable = layer4.0.bn1.weight, Gradient requires_grad = True
variable = layer4.0.bn1.bias, Gradient requires_grad = True
variable = layer4.0.conv2.weight, Gradient requires_grad = False
variable = layer4.0.conv2.popup_scores, Gradient requires_grad = True
variable = layer4.0.bn2.weight, Gradient requires_grad = True
variable = layer4.0.bn2.bias, Gradient requires_grad = True
variable = layer4.0.shortcut.0.weight, Gradient requires_grad = False
variable = layer4.0.shortcut.0.popup_scores, Gradient requires_grad = True
variable = layer4.0.shortcut.1.weight, Gradient requires_grad = True
variable = layer4.0.shortcut.1.bias, Gradient requires_grad = True
variable = layer4.1.conv1.weight, Gradient requires_grad = False
variable = layer4.1.conv1.popup_scores, Gradient requires_grad = True
variable = layer4.1.bn1.weight, Gradient requires_grad = True
variable = layer4.1.bn1.bias, Gradient requires_grad = True
variable = layer4.1.conv2.weight, Gradient requires_grad = False
variable = layer4.1.conv2.popup_scores, Gradient requires_grad = True
variable = layer4.1.bn2.weight, Gradient requires_grad = True
variable = layer4.1.bn2.bias, Gradient requires_grad = True
variable = linear.weight, Gradient requires_grad = False
variable = linear.bias, Gradient requires_grad = False
variable = linear.popup_scores, Gradient requires_grad = True
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.09938
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [0][  0/196]	Time  3.500 ( 3.500)	Data  0.000 ( 0.000)	Loss 1.7304 (1.7304)	Acc_1  48.83 ( 48.83)	Acc_5  89.45 ( 89.45)
Epoch: [0][100/196]	Time  3.679 ( 3.677)	Data  0.000 ( 0.000)	Loss 1.4681 (1.5582)	Acc_1  62.89 ( 59.29)	Acc_5  97.66 ( 95.69)
Test: [39/40]	Time  0.542 ( 2.299)	Loss 1.1252 (1.2655)	Adv_Loss 1.5735 (1.7707)	Acc_1  68.75 ( 60.93)	Acc_5 100.00 ( 95.36)	Adv-Acc_1  37.50 ( 33.23)	Adv-Acc_5  93.75 ( 85.39)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.10000
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [1][  0/196]	Time  3.481 ( 3.481)	Data  0.000 ( 0.000)	Loss 1.5065 (1.5065)	Acc_1  60.16 ( 60.16)	Acc_5  94.92 ( 94.92)
Epoch: [1][100/196]	Time  3.679 ( 3.676)	Data  0.000 ( 0.000)	Loss 1.4108 (1.4859)	Acc_1  69.92 ( 63.71)	Acc_5  96.48 ( 96.33)
Test: [39/40]	Time  0.543 ( 2.300)	Loss 1.1454 (1.2318)	Adv_Loss 1.5822 (1.7167)	Acc_1  62.50 ( 63.18)	Acc_5 100.00 ( 96.44)	Adv-Acc_1  31.25 ( 35.19)	Adv-Acc_5  93.75 ( 86.71)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.09938
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [2][  0/196]	Time  3.493 ( 3.493)	Data  0.000 ( 0.000)	Loss 1.5146 (1.5146)	Acc_1  62.11 ( 62.11)	Acc_5  96.48 ( 96.48)
Epoch: [2][100/196]	Time  3.675 ( 3.677)	Data  0.000 ( 0.000)	Loss 1.4617 (1.4669)	Acc_1  64.06 ( 65.18)	Acc_5  97.66 ( 96.69)
Test: [39/40]	Time  0.543 ( 2.300)	Loss 0.9437 (1.2303)	Adv_Loss 1.3782 (1.7843)	Acc_1  62.50 ( 60.16)	Acc_5 100.00 ( 95.88)	Adv-Acc_1  31.25 ( 32.61)	Adv-Acc_5 100.00 ( 84.42)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.09755
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [3][  0/196]	Time  3.514 ( 3.514)	Data  0.000 ( 0.000)	Loss 1.3750 (1.3750)	Acc_1  68.75 ( 68.75)	Acc_5  96.88 ( 96.88)
Epoch: [3][100/196]	Time  3.694 ( 3.696)	Data  0.000 ( 0.000)	Loss 1.4691 (1.4521)	Acc_1  64.45 ( 65.73)	Acc_5  96.88 ( 96.76)
Test: [39/40]	Time  0.546 ( 2.313)	Loss 1.0393 (1.2048)	Adv_Loss 1.4499 (1.6762)	Acc_1  75.00 ( 65.06)	Acc_5 100.00 ( 96.66)	Adv-Acc_1  31.25 ( 37.31)	Adv-Acc_5  93.75 ( 87.24)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.09455
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [4][  0/196]	Time  3.479 ( 3.479)	Data  0.000 ( 0.000)	Loss 1.4372 (1.4372)	Acc_1  66.41 ( 66.41)	Acc_5  96.88 ( 96.88)
Epoch: [4][100/196]	Time  3.695 ( 3.695)	Data  0.000 ( 0.000)	Loss 1.4841 (1.4444)	Acc_1  61.72 ( 66.15)	Acc_5  94.92 ( 96.90)
Test: [39/40]	Time  0.549 ( 2.313)	Loss 1.0538 (1.1931)	Adv_Loss 1.5001 (1.6837)	Acc_1  75.00 ( 65.70)	Acc_5 100.00 ( 96.45)	Adv-Acc_1  31.25 ( 37.45)	Adv-Acc_5 100.00 ( 86.89)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.09045
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [5][  0/196]	Time  3.466 ( 3.466)	Data  0.000 ( 0.000)	Loss 1.3568 (1.3568)	Acc_1  73.44 ( 73.44)	Acc_5  99.22 ( 99.22)
Epoch: [5][100/196]	Time  3.670 ( 3.672)	Data  0.000 ( 0.000)	Loss 1.5726 (1.4315)	Acc_1  60.55 ( 66.80)	Acc_5  95.31 ( 97.11)
Test: [39/40]	Time  0.530 ( 2.297)	Loss 0.9933 (1.2048)	Adv_Loss 1.4137 (1.6989)	Acc_1  75.00 ( 64.99)	Acc_5 100.00 ( 96.35)	Adv-Acc_1  50.00 ( 36.95)	Adv-Acc_5 100.00 ( 85.93)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.08536
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [6][  0/196]	Time  3.442 ( 3.442)	Data  0.000 ( 0.000)	Loss 1.3722 (1.3722)	Acc_1  69.92 ( 69.92)	Acc_5  97.27 ( 97.27)
Epoch: [6][100/196]	Time  3.670 ( 3.671)	Data  0.000 ( 0.000)	Loss 1.5240 (1.4219)	Acc_1  64.45 ( 67.36)	Acc_5  94.92 ( 97.11)
Test: [39/40]	Time  0.531 ( 2.297)	Loss 1.0382 (1.2210)	Adv_Loss 1.4737 (1.7171)	Acc_1  68.75 ( 63.65)	Acc_5 100.00 ( 96.29)	Adv-Acc_1  25.00 ( 34.95)	Adv-Acc_5 100.00 ( 86.58)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.07939
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [7][  0/196]	Time  3.470 ( 3.470)	Data  0.000 ( 0.000)	Loss 1.4304 (1.4304)	Acc_1  61.72 ( 61.72)	Acc_5  96.88 ( 96.88)
Epoch: [7][100/196]	Time  3.672 ( 3.671)	Data  0.000 ( 0.000)	Loss 1.4377 (1.4226)	Acc_1  66.02 ( 67.20)	Acc_5  97.66 ( 97.22)
Test: [39/40]	Time  0.532 ( 2.296)	Loss 0.9235 (1.1674)	Adv_Loss 1.3558 (1.6866)	Acc_1  87.50 ( 65.99)	Acc_5 100.00 ( 97.01)	Adv-Acc_1  31.25 ( 35.67)	Adv-Acc_5 100.00 ( 86.77)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.07270
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [8][  0/196]	Time  3.444 ( 3.444)	Data  0.000 ( 0.000)	Loss 1.4201 (1.4201)	Acc_1  65.23 ( 65.23)	Acc_5  98.05 ( 98.05)
Epoch: [8][100/196]	Time  3.668 ( 3.671)	Data  0.000 ( 0.000)	Loss 1.3964 (1.4230)	Acc_1  75.39 ( 67.87)	Acc_5  98.05 ( 97.11)
Test: [39/40]	Time  0.529 ( 2.297)	Loss 0.9586 (1.1839)	Adv_Loss 1.3941 (1.6862)	Acc_1  75.00 ( 64.62)	Acc_5 100.00 ( 96.34)	Adv-Acc_1  37.50 ( 37.45)	Adv-Acc_5 100.00 ( 86.17)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.06545
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [9][  0/196]	Time  3.476 ( 3.476)	Data  0.000 ( 0.000)	Loss 1.4193 (1.4193)	Acc_1  67.97 ( 67.97)	Acc_5  98.05 ( 98.05)
Epoch: [9][100/196]	Time  3.672 ( 3.671)	Data  0.000 ( 0.000)	Loss 1.4461 (1.4148)	Acc_1  65.62 ( 67.91)	Acc_5  96.48 ( 97.26)
Test: [39/40]	Time  0.532 ( 2.296)	Loss 0.9808 (1.1708)	Adv_Loss 1.3963 (1.6512)	Acc_1  81.25 ( 65.57)	Acc_5 100.00 ( 96.85)	Adv-Acc_1  37.50 ( 38.61)	Adv-Acc_5  93.75 ( 87.93)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.05782
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [10][  0/196]	Time  3.498 ( 3.498)	Data  0.000 ( 0.000)	Loss 1.3052 (1.3052)	Acc_1  71.48 ( 71.48)	Acc_5  98.05 ( 98.05)
Epoch: [10][100/196]	Time  3.676 ( 3.677)	Data  0.000 ( 0.000)	Loss 1.3805 (1.4073)	Acc_1  67.58 ( 68.21)	Acc_5  97.66 ( 97.23)
Test: [39/40]	Time  0.536 ( 2.301)	Loss 1.0210 (1.1738)	Adv_Loss 1.4563 (1.6608)	Acc_1  68.75 ( 65.87)	Acc_5 100.00 ( 96.72)	Adv-Acc_1  31.25 ( 38.00)	Adv-Acc_5  93.75 ( 87.77)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.05000
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [11][  0/196]	Time  3.445 ( 3.445)	Data  0.000 ( 0.000)	Loss 1.4108 (1.4108)	Acc_1  67.97 ( 67.97)	Acc_5  96.88 ( 96.88)
Epoch: [11][100/196]	Time  3.673 ( 3.673)	Data  0.000 ( 0.000)	Loss 1.3744 (1.4039)	Acc_1  68.75 ( 68.85)	Acc_5  98.05 ( 97.22)
Test: [39/40]	Time  0.533 ( 2.300)	Loss 0.9561 (1.1542)	Adv_Loss 1.3891 (1.6442)	Acc_1  75.00 ( 67.73)	Acc_5 100.00 ( 96.87)	Adv-Acc_1  43.75 ( 38.70)	Adv-Acc_5 100.00 ( 87.51)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.04218
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [12][  0/196]	Time  3.493 ( 3.493)	Data  0.000 ( 0.000)	Loss 1.4447 (1.4447)	Acc_1  71.88 ( 71.88)	Acc_5  96.88 ( 96.88)
Epoch: [12][100/196]	Time  3.679 ( 3.676)	Data  0.000 ( 0.000)	Loss 1.3998 (1.3939)	Acc_1  66.80 ( 69.23)	Acc_5  97.66 ( 97.40)
Test: [39/40]	Time  0.534 ( 2.301)	Loss 0.8855 (1.1675)	Adv_Loss 1.2951 (1.6785)	Acc_1  81.25 ( 66.63)	Acc_5 100.00 ( 96.44)	Adv-Acc_1  50.00 ( 37.18)	Adv-Acc_5  93.75 ( 86.61)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.03455
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [13][  0/196]	Time  3.451 ( 3.451)	Data  0.000 ( 0.000)	Loss 1.3335 (1.3335)	Acc_1  70.70 ( 70.70)	Acc_5  98.05 ( 98.05)
Epoch: [13][100/196]	Time  3.674 ( 3.676)	Data  0.000 ( 0.000)	Loss 1.4386 (1.3876)	Acc_1  63.67 ( 69.22)	Acc_5  96.09 ( 97.50)
Test: [39/40]	Time  0.535 ( 2.300)	Loss 0.9397 (1.1435)	Adv_Loss 1.3620 (1.6278)	Acc_1  75.00 ( 66.49)	Acc_5 100.00 ( 96.91)	Adv-Acc_1  50.00 ( 39.40)	Adv-Acc_5 100.00 ( 87.81)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.02730
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [14][  0/196]	Time  3.497 ( 3.497)	Data  0.000 ( 0.000)	Loss 1.4675 (1.4675)	Acc_1  62.89 ( 62.89)	Acc_5  95.31 ( 95.31)
Epoch: [14][100/196]	Time  3.677 ( 3.676)	Data  0.000 ( 0.000)	Loss 1.3065 (1.3862)	Acc_1  72.66 ( 69.50)	Acc_5  97.66 ( 97.42)
Test: [39/40]	Time  0.534 ( 2.300)	Loss 1.0348 (1.1737)	Adv_Loss 1.5140 (1.6685)	Acc_1  75.00 ( 66.88)	Acc_5 100.00 ( 96.59)	Adv-Acc_1  31.25 ( 37.99)	Adv-Acc_5  93.75 ( 87.57)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.02061
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [15][  0/196]	Time  3.438 ( 3.438)	Data  0.000 ( 0.000)	Loss 1.3144 (1.3144)	Acc_1  72.27 ( 72.27)	Acc_5  98.05 ( 98.05)
Epoch: [15][100/196]	Time  3.673 ( 3.671)	Data  0.000 ( 0.000)	Loss 1.3734 (1.3839)	Acc_1  69.14 ( 69.77)	Acc_5  97.66 ( 97.56)
Test: [39/40]	Time  0.529 ( 2.297)	Loss 0.9471 (1.1260)	Adv_Loss 1.3697 (1.6332)	Acc_1  81.25 ( 67.27)	Acc_5 100.00 ( 97.20)	Adv-Acc_1  37.50 ( 38.65)	Adv-Acc_5 100.00 ( 87.69)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.01464
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [16][  0/196]	Time  3.483 ( 3.483)	Data  0.000 ( 0.000)	Loss 1.3538 (1.3538)	Acc_1  76.17 ( 76.17)	Acc_5  96.88 ( 96.88)
Epoch: [16][100/196]	Time  3.666 ( 3.672)	Data  0.000 ( 0.000)	Loss 1.4488 (1.3787)	Acc_1  66.80 ( 70.08)	Acc_5  96.48 ( 97.53)
Test: [39/40]	Time  0.531 ( 2.297)	Loss 0.9744 (1.1898)	Adv_Loss 1.4286 (1.6998)	Acc_1  75.00 ( 63.72)	Acc_5 100.00 ( 96.42)	Adv-Acc_1  50.00 ( 35.58)	Adv-Acc_5  87.50 ( 87.30)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.00955
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [17][  0/196]	Time  3.444 ( 3.444)	Data  0.000 ( 0.000)	Loss 1.3132 (1.3132)	Acc_1  70.70 ( 70.70)	Acc_5  98.05 ( 98.05)
Epoch: [17][100/196]	Time  3.674 ( 3.671)	Data  0.000 ( 0.000)	Loss 1.3858 (1.3798)	Acc_1  72.66 ( 70.09)	Acc_5  98.05 ( 97.70)
Test: [39/40]	Time  0.531 ( 2.297)	Loss 0.8819 (1.0943)	Adv_Loss 1.3146 (1.6208)	Acc_1  75.00 ( 68.75)	Acc_5 100.00 ( 97.19)	Adv-Acc_1  50.00 ( 39.43)	Adv-Acc_5 100.00 ( 87.82)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.00545
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [18][  0/196]	Time  3.481 ( 3.481)	Data  0.000 ( 0.000)	Loss 1.3692 (1.3692)	Acc_1  70.31 ( 70.31)	Acc_5  98.83 ( 98.83)
Epoch: [18][100/196]	Time  3.673 ( 3.671)	Data  0.000 ( 0.000)	Loss 1.3378 (1.3658)	Acc_1  69.92 ( 70.56)	Acc_5  96.88 ( 97.57)
Test: [39/40]	Time  0.531 ( 2.297)	Loss 0.9407 (1.1259)	Adv_Loss 1.3595 (1.6218)	Acc_1  81.25 ( 68.54)	Acc_5 100.00 ( 97.30)	Adv-Acc_1  43.75 ( 39.20)	Adv-Acc_5  93.75 ( 88.50)
 ->->->->->->->->->-> One epoch with Adversarial training (TRADES) <-<-<-<-<-<-<-<-<-<-
torch.Size([256, 3, 32, 32]) torch.Size([256]) Batch_size from args: 256 lr: 0.00245
Training images range: [tensor(0., device='cuda:0'), tensor(1., device='cuda:0')]
Epoch: [19][  0/196]	Time  3.494 ( 3.494)	Data  0.000 ( 0.000)	Loss 1.4788 (1.4788)	Acc_1  67.58 ( 67.58)	Acc_5  96.88 ( 96.88)
Epoch: [19][100/196]	Time  3.672 ( 3.671)	Data  0.000 ( 0.000)	Loss 1.3961 (1.3749)	Acc_1  72.66 ( 70.65)	Acc_5  97.27 ( 97.71)
Test: [39/40]	Time  0.530 ( 2.296)	Loss 0.9222 (1.1361)	Adv_Loss 1.3531 (1.6244)	Acc_1  75.00 ( 68.98)	Acc_5 100.00 ( 97.15)	Adv-Acc_1  43.75 ( 39.02)	Adv-Acc_5  93.75 ( 88.39)
conv1 SubnetConv(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.98842592592592
layer1.0.conv1 SubnetConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.99837239583333
layer1.0.conv2 SubnetConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.99837239583333
layer1.1.conv1 SubnetConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.99837239583333
layer1.1.conv2 SubnetConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.99837239583333
layer2.0.conv1 SubnetConv(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) 89.99972873263889
layer2.0.conv2 SubnetConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.99972873263889
layer2.0.shortcut.0 SubnetConv(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) 89.990234375
layer2.1.conv1 SubnetConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.99972873263889
layer2.1.conv2 SubnetConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.99972873263889
layer3.0.conv1 SubnetConv(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) 89.99972873263889
layer3.0.conv2 SubnetConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.99989827473958
layer3.0.shortcut.0 SubnetConv(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) 89.9993896484375
layer3.1.conv1 SubnetConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.99989827473958
layer3.1.conv2 SubnetConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.99989827473958
layer4.0.conv1 SubnetConv(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) 89.99998304578993
layer4.0.conv2 SubnetConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.99998304578993
layer4.0.shortcut.0 SubnetConv(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) 89.9993896484375
layer4.1.conv1 SubnetConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.99998304578993
layer4.1.conv2 SubnetConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 89.99998304578993
linear SubnetLinear(in_features=512, out_features=10, bias=True) 90.0
